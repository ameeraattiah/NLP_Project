{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "366e2312-48c7-4df4-8121-74ab7a7c995e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import json\n",
    "import fasttext\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from trafilatura import extract\n",
    "from bs4 import BeautifulSoup\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from datatrove.pipeline.filters import (\n",
    "    GopherQualityFilter,\n",
    "    FineWebQualityFilter,\n",
    "    C4QualityFilter,\n",
    "    GopherRepetitionFilter,\n",
    ")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from emot.emo_unicode import UNICODE_EMO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2f59e91a-8b84-4329-84ac-3ed232247423",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mock Document class to wrap text for DataTrove filters\n",
    "class Document:\n",
    "    def __init__(self, text):\n",
    "        self.text = text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a6d71609-2938-43b7-9ced-09c4ec13e277",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load FastText model\n",
    "FASTTEXT_MODEL_PATH = \"lid.176.bin\"\n",
    "language_model = fasttext.load_model(FASTTEXT_MODEL_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4b2b7098-afae-4a92-bd19-5192e8585eba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize DataTrove filters\n",
    "gopher_filter = GopherQualityFilter()\n",
    "fineweb_filter = FineWebQualityFilter()\n",
    "c4_filter = C4QualityFilter()\n",
    "repetition_filter = GopherRepetitionFilter()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "19062eac-f94b-416e-b274-113515dee233",
   "metadata": {},
   "outputs": [],
   "source": [
    "# UT1 Blocklist\n",
    "UT1_BLOCKLIST_URL = \"http://dsi.ut-capitole.fr/blacklists/download/blacklists.tar.gz\"\n",
    "ut1_text_keywords = []\n",
    "ut1_link_keywords = []\n",
    "\n",
    "def load_ut1_blocklist():\n",
    "    \"\"\"Download and parse UT1 blocklist.\"\"\"\n",
    "    global ut1_text_keywords, ut1_link_keywords\n",
    "    try:\n",
    "        response = requests.get(UT1_BLOCKLIST_URL)\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading UT1 blocklist: {e}\")\n",
    "        ut1_text_keywords = []\n",
    "        ut1_link_keywords = []\n",
    "\n",
    "def is_blocklisted(content, source_url):\n",
    "    \"\"\"Check if content or URL contains blocklisted words or links.\"\"\"\n",
    "    if any(keyword in content for keyword in ut1_text_keywords):\n",
    "        return True\n",
    "    if source_url and any(link in source_url for link in ut1_link_keywords):\n",
    "        return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0db6131d-7c4c-4623-87d8-0eea03590308",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_html(content):\n",
    "    \"\"\"Remove HTML tags from content.\"\"\"\n",
    "    if \"<html\" in content.lower() or \"<body\" in content.lower():\n",
    "        soup = BeautifulSoup(content, \"html.parser\")\n",
    "        return soup.get_text(separator=\" \").strip()\n",
    "    return content.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "7107281d-c102-4d63-8f2c-3729726c8419",
   "metadata": {},
   "outputs": [],
   "source": [
    "def detect_language(text):\n",
    "    \"\"\"Detect the language of the text using FastText.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    prediction = language_model.predict(cleaned_text[:1000])\n",
    "    language = prediction[0][0].replace(\"__label__\", \"\")\n",
    "    confidence = prediction[1][0]\n",
    "    return language, confidence"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "6eab7438-9257-4622-9015-e934eae410ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_non_arabic_text(text):\n",
    "    \"\"\"Remove non-Arabic text using FastText language detection.\"\"\"\n",
    "    sentences = text.split(\"\\n\")\n",
    "    arabic_sentences = [\n",
    "        sentence for sentence in sentences if detect_language(sentence)[0] == \"ar\"\n",
    "    ]\n",
    "    return \"\\n\".join(arabic_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e08e9c4-264f-430c-b621-5f65aa291f23",
   "metadata": {},
   "outputs": [],
   "source": [
    "def has_excessive_newlines(text, threshold=0.5):\n",
    "    \"\"\"Check if the text has excessive newlines compared to its word count.\"\"\"\n",
    "    newline_count = text.count(\"\\n\")\n",
    "    word_count = len(text.split())\n",
    "    return newline_count > word_count * threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "6aac37de-34d0-4bd0-bfae-511f872e3e8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_emojis(text):\n",
    "    \"\"\"Replace emojis with descriptive text.\"\"\"\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(\n",
    "            emot,\n",
    "            \" \".join(UNICODE_EMO[emot].replace(\",\", \" \").replace(\":\", \" \").split()),\n",
    "        )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "23ab182f-58b9-49dc-b33b-7e9b6732650f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_text(text):\n",
    "    \"\"\"Normalize Arabic text by removing diacritics and cleaning up.\"\"\"\n",
    "    text = re.sub(r\"[ًٌٍَُِّْ]\", \"\", text)  # Remove Arabic diacritics\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    return text.strip()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "304bd043-904a-46e2-a4b2-65bec9493058",
   "metadata": {},
   "outputs": [],
   "source": [
    "def deduplicate_documents(data, threshold=0.8):\n",
    "    \"\"\"Remove duplicates at the document level using MinHash.\"\"\"\n",
    "    lsh = MinHashLSH(threshold=threshold)\n",
    "    unique_data = []\n",
    "    for idx, record in enumerate(data):\n",
    "        text = record['text']\n",
    "        tokens = word_tokenize(text)\n",
    "        m = MinHash()\n",
    "        for token in tokens:\n",
    "            m.update(token.encode(\"utf-8\"))\n",
    "        if not any(lsh.query(m)):\n",
    "            lsh.insert(str(idx), m)\n",
    "            unique_data.append(record)\n",
    "    return unique_data\n",
    "\n",
    "def deduplicate_sentences(text):\n",
    "    \"\"\"Remove duplicate sentences within a single document.\"\"\"\n",
    "    sentences = text.split(\"\\n\")\n",
    "    unique_sentences = list(dict.fromkeys(sentences))  # Preserve order\n",
    "    return \"\\n\".join(unique_sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "26558d08-c625-4a32-bd9a-2d085b9566b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def is_high_quality_text(text):\n",
    "    \"\"\"Filter out low-quality text (e.g., too short or mostly whitespace).\"\"\"\n",
    "    if len(text.split()) < 4 or text.strip().count(\"\\n\") > len(text.split()) * 0.5:\n",
    "        return False\n",
    "    return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "24e9ec86-0feb-47d1-8df9-68b551fa2ddc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pipeline_multiple(warc_file_paths, output_folder=\"Output\", max_records=1000):\n",
    "    \"\"\"Main pipeline function to process multiple WARC files.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    processed_data = []\n",
    "    total_records = 0\n",
    "\n",
    "    for warc_file_path in warc_file_paths:\n",
    "        with open(warc_file_path, \"rb\") as stream:\n",
    "            for record in ArchiveIterator(stream):\n",
    "                if record.rec_type == \"response\":\n",
    "                    content = record.content_stream().read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                    source_url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "                    date = record.rec_headers.get_header(\"WARC-Date\")\n",
    "\n",
    "                    # Extract meaningful text with Trafilatura\n",
    "                    extracted_text = extract(content)\n",
    "                    if not extracted_text:\n",
    "                        continue\n",
    "\n",
    "                    # Clean HTML content\n",
    "                    cleaned_text = clean_html(extracted_text)\n",
    "\n",
    "                    # Detect primary language\n",
    "                    language, confidence = detect_language(cleaned_text)\n",
    "                    if language != \"ar\" or confidence < 0.95:\n",
    "                        continue\n",
    "\n",
    "                    # Remove non-Arabic text\n",
    "                    arabic_only_text = remove_non_arabic_text(cleaned_text)\n",
    "\n",
    "                    # Check for excessive newlines\n",
    "                    if has_excessive_newlines(arabic_only_text):\n",
    "                        print(f\"Skipping source due to excessive newlines: {source_url}\")\n",
    "                        continue\n",
    "\n",
    "                    # Blocklist filtering\n",
    "                    if is_blocklisted(arabic_only_text, source_url):\n",
    "                        continue\n",
    "\n",
    "                    # Normalize, remove emojis, and deduplicate sentences\n",
    "                    normalized_text = normalize_text(convert_emojis(arabic_only_text))\n",
    "                    deduplicated_text = deduplicate_sentences(normalized_text)\n",
    "\n",
    "                    # Wrap text in a mock `Document` object\n",
    "                    document = Document(text=deduplicated_text)\n",
    "\n",
    "                    # Apply quality filters\n",
    "                    if not gopher_filter.filter(document):\n",
    "                        continue\n",
    "                    if not fineweb_filter.filter(document):\n",
    "                        continue\n",
    "                    if not c4_filter.filter(document):\n",
    "                        continue\n",
    "\n",
    "                    # Add metadata\n",
    "                    metadata = {\n",
    "                        \"date\": date,\n",
    "                        \"labels\": {\n",
    "                            \"language\": language,\n",
    "                            \"language_score\": confidence,\n",
    "                        },\n",
    "                        \"source\": source_url,\n",
    "                        \"token_count\": len(deduplicated_text.split()),\n",
    "                    }\n",
    "\n",
    "                    processed_data.append({\"text\": deduplicated_text, \"metadata\": metadata})\n",
    "                    total_records += 1\n",
    "\n",
    "                    if total_records >= max_records:\n",
    "                        print(f\"Reached maximum records limit ({max_records}). Stopping.\")\n",
    "                        break\n",
    "            if total_records >= max_records:\n",
    "                break\n",
    "\n",
    "    # Deduplicate across documents\n",
    "    processed_data = deduplicate_documents(processed_data)\n",
    "\n",
    "    # Save processed data to a JSON file\n",
    "    output_file_path = os.path.join(output_folder, \"processed_texts_with_metadata.json\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(processed_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Processed {total_records} Arabic texts successfully across all WARC files.\")\n",
    "    return output_file_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "9ae5ba7d-b22d-4abe-9065-2685cac041b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping source due to excessive newlines: https://ankarayuva.com/%D8%AA%D9%88%D8%A7%D8%B5%D9%84-%D9%85%D8%B9%D9%86%D8%A7/\n",
      "Skipping source due to excessive newlines: http://aphysem.ma/ar/%D9%85%D9%83%D8%AA%D8%A8%D8%A9-%D8%A7%D9%84%D9%88%D8%B3%D8%A7%D8%A6%D8%B7/\n",
      "Skipping source due to excessive newlines: https://baxemo.net/category/furniture-appliances/furniture-tableware\n",
      "Skipping source due to excessive newlines: https://www.al-jazirah.com/2024/20240920/adv.htm\n",
      "Processed 308 Arabic texts successfully across all WARC files.\n",
      "Output saved to Output/processed_texts_with_metadata.json\n"
     ]
    }
   ],
   "source": [
    "# Example usage with multiple WARC files\n",
    "load_ut1_blocklist()\n",
    "warc_file_paths = [\n",
    "    \"CC-MAIN-2024-42/downloaded_warc_files/CC-MAIN-20241003094020-20241003124020-00006.warc.gz\",\n",
    "    \"CC-MAIN-2024-42/downloaded_warc_files/CC-MAIN-20241003094020-20241003124020-00017.warc.gz\"\n",
    "]\n",
    "output_file = process_pipeline_multiple(warc_file_paths)\n",
    "print(f\"Output saved to {output_file}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
