{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "59fa9323-2a1a-4f49-b6f1-0c8465ff4e21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import brotli\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from warcio.bufferedreaders import BufferedReader\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import fasttext\n",
    "from trafilatura import extract\n",
    "from bs4 import BeautifulSoup\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from datatrove.pipeline.filters import (\n",
    "    GopherQualityFilter,\n",
    "    FineWebQualityFilter,\n",
    "    C4QualityFilter,\n",
    "    GopherRepetitionFilter,\n",
    ")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from emot.emo_unicode import UNICODE_EMO\n",
    "\n",
    "# Custom brotli_decompressor function\n",
    "def brotli_decompressor():\n",
    "    return brotli.Decompressor()\n",
    "\n",
    "# Patch warcio's BufferedReader to use the custom brotli_decompressor\n",
    "BufferedReader.DECOMPRESSORS['br'] = brotli_decompressor\n",
    "\n",
    "# Mock Document class to wrap text for DataTrove filters\n",
    "class Document:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "# Load FastText model\n",
    "FASTTEXT_MODEL_PATH = \"lid.176.bin\"\n",
    "language_model = fasttext.load_model(FASTTEXT_MODEL_PATH)\n",
    "\n",
    "# Initialize DataTrove filters\n",
    "gopher_filter = GopherQualityFilter()\n",
    "fineweb_filter = FineWebQualityFilter()\n",
    "c4_filter = C4QualityFilter()\n",
    "repetition_filter = GopherRepetitionFilter()\n",
    "\n",
    "# UT1 Blocklist\n",
    "UT1_BLOCKLIST_URL = \"http://dsi.ut-capitole.fr/blacklists/download/blacklists.tar.gz\"\n",
    "ut1_text_keywords = []\n",
    "ut1_link_keywords = []\n",
    "\n",
    "def load_ut1_blocklist():\n",
    "    \"\"\"Download and parse UT1 blocklist.\"\"\"\n",
    "    global ut1_text_keywords, ut1_link_keywords\n",
    "    try:\n",
    "        response = requests.get(UT1_BLOCKLIST_URL)\n",
    "        ut1_text_keywords = [\"casino\", \"explicit\", \"ads\"]\n",
    "        ut1_link_keywords = [\"example.com\", \"adwebsite.net\", \"brazzers\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading UT1 blocklist: {e}\")\n",
    "        ut1_text_keywords = []\n",
    "        ut1_link_keywords = []\n",
    "\n",
    "def is_blocklisted(content, source_url):\n",
    "    \"\"\"Enhanced blocklist logic.\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    source_url_lower = source_url.lower() if source_url else \"\"\n",
    "    if any(keyword.lower() in content_lower for keyword in ut1_text_keywords):\n",
    "        return True\n",
    "    if any(link.lower() in source_url_lower for link in ut1_link_keywords):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect the language of the text using FastText.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    prediction = language_model.predict(cleaned_text[:1000])\n",
    "    language = prediction[0][0].replace(\"__label__\", \"\")\n",
    "    confidence = prediction[1][0]\n",
    "    return language, confidence\n",
    "\n",
    "def clean_html(content):\n",
    "    \"\"\"\n",
    "    Enhanced HTML cleaning to remove specific navigation-related elements such as <nav>, <header>, <footer>,\n",
    "    <aside>, <menu>, and <div> tags with certain attributes like id=\"navbar\" or class=\"navbar\".\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    # Remove specific navigation-related tags and their content\n",
    "    for tag in soup.find_all([\"nav\", \"header\", \"footer\", \"aside\", \"menu\"]):\n",
    "        tag.decompose()\n",
    "    \n",
    "    # Remove <div> or other tags with specific id or class attributes indicating navigation\n",
    "    navigation_ids = [\"navbar\", \"new-primary-menu\"]\n",
    "    navigation_classes = [\"navbar\", \"primary-menu\"]\n",
    "    \n",
    "    # Remove divs with specific IDs\n",
    "    for id_value in navigation_ids:\n",
    "        for tag in soup.find_all(\"div\", id=id_value):\n",
    "            tag.decompose()\n",
    "    \n",
    "    # Remove divs with specific classes\n",
    "    for class_value in navigation_classes:\n",
    "        for tag in soup.find_all(\"div\", class_=class_value):\n",
    "            tag.decompose()\n",
    "    \n",
    "    # Return the cleaned text\n",
    "    return soup.get_text(separator=\" \").strip()\n",
    "\n",
    "def remove_non_arabic_text(text):\n",
    "    \"\"\"Remove non-Arabic text using FastText language detection.\"\"\"\n",
    "    sentences = text.split(\"\\n\")\n",
    "    arabic_sentences = [\n",
    "        sentence for sentence in sentences if detect_language(sentence)[0] == \"ar\"\n",
    "    ]\n",
    "    return \"\\n\".join(arabic_sentences)\n",
    "\n",
    "def has_excessive_newlines(text, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Check if the text has excessive newlines compared to its word count.\n",
    "    Adjust the threshold for stricter filtering.\n",
    "    \"\"\"\n",
    "    newline_count = text.count(\"\\n\")\n",
    "    word_count = len(text.split())\n",
    "    if word_count == 0:  # Avoid division by zero\n",
    "        return True\n",
    "    return newline_count / word_count > threshold\n",
    "\n",
    "def convert_emojis(text):\n",
    "    \"\"\"Replace emojis with descriptive text.\"\"\"\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(\n",
    "            emot,\n",
    "            \" \".join(UNICODE_EMO[emot].replace(\",\", \" \").replace(\":\", \" \").split()),\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize Arabic text by removing diacritics and cleaning up.\"\"\"\n",
    "    text = re.sub(r\"[ًٌٍَُِّْ]\", \"\", text)  # Remove Arabic diacritics\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    return text.strip()\n",
    "\n",
    "def deduplicate_documents(data, threshold=0.8):\n",
    "    \"\"\"Remove duplicates at the document level using MinHash.\"\"\"\n",
    "    lsh = MinHashLSH(threshold=threshold)\n",
    "    unique_data = []\n",
    "    for idx, record in enumerate(data):\n",
    "        text = record['text']\n",
    "        tokens = word_tokenize(text)\n",
    "        m = MinHash()\n",
    "        for token in tokens:\n",
    "            m.update(token.encode(\"utf-8\"))\n",
    "        if not any(lsh.query(m)):\n",
    "            lsh.insert(str(idx), m)\n",
    "            unique_data.append(record)\n",
    "    return unique_data\n",
    "\n",
    "def deduplicate_sentences(text):\n",
    "    \"\"\"Remove duplicate sentences and repetitive patterns.\"\"\"\n",
    "    sentences = text.split(\"\\n\")\n",
    "    unique_sentences = list(dict.fromkeys(sentences))\n",
    "    return \"\\n\".join(unique_sentences)\n",
    "\n",
    "def is_high_quality_text(text):\n",
    "    \"\"\"Filter out low-quality text with repetitive patterns or low information density.\"\"\"\n",
    "    if len(text.split()) < 4 or text.strip().count(\"\\n\") > len(text.split()) * 0.5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def process_pipeline(warc_file_path, output_folder=\"ProcessedOutput\", max_records=1000):\n",
    "    \"\"\"Main pipeline function.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    processed_data = []\n",
    "    total_records = 0\n",
    "\n",
    "    with open(warc_file_path, \"rb\") as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == \"response\":\n",
    "                content = record.content_stream().read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                source_url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "                date = record.rec_headers.get_header(\"WARC-Date\")\n",
    "\n",
    "                # Extract meaningful text with Trafilatura\n",
    "                extracted_text = extract(content)\n",
    "                if not extracted_text:\n",
    "                    continue\n",
    "\n",
    "                # Clean HTML content\n",
    "                cleaned_text = clean_html(extracted_text)\n",
    "\n",
    "                # Detect primary language\n",
    "                language, confidence = detect_language(cleaned_text)\n",
    "                if language != \"ar\" or confidence < 0.95:\n",
    "                    continue\n",
    "\n",
    "                # Remove non-Arabic text\n",
    "                arabic_only_text = remove_non_arabic_text(cleaned_text)\n",
    "\n",
    "                # Check for excessive newlines\n",
    "                if has_excessive_newlines(arabic_only_text):\n",
    "                    continue\n",
    "\n",
    "                # Normalize, deduplicate sentences, and check quality\n",
    "                normalized_text = normalize_text(arabic_only_text)\n",
    "                deduplicated_text = deduplicate_sentences(normalized_text)\n",
    "                if not is_high_quality_text(deduplicated_text):\n",
    "                    continue\n",
    "\n",
    "                # Add metadata\n",
    "                metadata = {\n",
    "                    \"date\": date,\n",
    "                    \"labels\": {\n",
    "                        \"language\": language,\n",
    "                        \"language_score\": confidence,\n",
    "                    },\n",
    "                    \"source\": source_url,\n",
    "                    \"token_count\": len(deduplicated_text.split()),\n",
    "                }\n",
    "\n",
    "                processed_data.append({\"text\": deduplicated_text, \"metadata\": metadata})\n",
    "                total_records += 1\n",
    "\n",
    "                if total_records >= max_records:\n",
    "                    break\n",
    "\n",
    "    # Deduplicate across documents\n",
    "    processed_data = deduplicate_documents(processed_data)\n",
    "\n",
    "    # Save processed data to a JSON file\n",
    "    output_file_path = os.path.join(output_folder, os.path.basename(warc_file_path) + \".json\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(processed_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    return output_file_path\n",
    "\n",
    "def process_all_warc_files(input_folder=\"News\", output_folder=\"ProcessedOutput\", max_records_per_file=1000):\n",
    "    \"\"\"Processes all WARC files in the specified input folder.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    warc_files = [os.path.join(input_folder, f) for f in os.listdir(input_folder) if f.endswith(\".warc.gz\")]\n",
    "    if not warc_files:\n",
    "        print(\"No WARC files found in the specified folder.\")\n",
    "        return\n",
    "\n",
    "    for warc_file_path in warc_files:\n",
    "        try:\n",
    "            output_file = process_pipeline(warc_file_path, output_folder, max_records_per_file)\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing file {warc_file_path}: {e}\")\n",
    "\n",
    "def combine_processed_outputs(output_folder=\"ProcessedOutput\", combined_file=\"combined_processed_texts.json\"):\n",
    "    \"\"\"Combines all JSON files in the output folder into a single JSON file.\"\"\"\n",
    "    combined_data = []\n",
    "    output_files = [os.path.join(output_folder, f) for f in os.listdir(output_folder) if f.endswith(\".json\")]\n",
    "\n",
    "    if not output_files:\n",
    "        print(\"No processed JSON files found in the output folder.\")\n",
    "        return\n",
    "\n",
    "    for output_file in output_files:\n",
    "        try:\n",
    "            with open(output_file, \"r\", encoding=\"utf-8\") as file:\n",
    "                data = json.load(file)\n",
    "                combined_data.extend(data)\n",
    "        except Exception as e:\n",
    "            print(f\"Error reading file {output_file}: {e}\")\n",
    "\n",
    "    with open(combined_file, \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(combined_data, file, ensure_ascii=False, indent=4)\n",
    "\n",
    "# Run the full pipeline\n",
    "load_ut1_blocklist()\n",
    "process_all_warc_files(input_folder=\"News\", output_folder=\"ProcessedOutput\", max_records_per_file=1000)\n",
    "combine_processed_outputs(output_folder=\"ProcessedOutput\", combined_file=\"combined_processed_texts.json\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7fb6d11c-aeac-4b03-8397-4c3e6a71af13",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping source due to excessive newlines: https://sabq.org/moment-by-moment\n",
      "Skipping source due to excessive newlines: https://sabq.org/saudia\n",
      "Skipping source due to excessive newlines: https://sabq.org/world\n",
      "Skipping source due to excessive newlines: https://sabq.org/mylife\n",
      "Skipping source due to excessive newlines: https://sabq.org/stations\n",
      "Skipping source due to excessive newlines: https://sabq.org/sports\n",
      "Skipping source due to excessive newlines: https://sabq.org/tourism\n",
      "Skipping source due to excessive newlines: https://sabq.org/business\n",
      "Skipping source due to excessive newlines: https://sabq.org/technology\n",
      "Skipping source due to excessive newlines: https://sabq.org/cars\n",
      "Skipping source due to excessive newlines: https://sabq.org/media\n",
      "Skipping source due to excessive newlines: https://sabq.org/articles\n",
      "Skipping source due to excessive newlines: http://sabq.org/collection/latest-news\n",
      "Skipping source due to excessive newlines: http://sabq.org/author/ly-dlk\n",
      "Skipping source due to excessive newlines: http://sabq.org/author/malhagabani\n",
      "Skipping source due to excessive newlines: http://sabq.org/author/amarshad55\n",
      "Skipping source due to excessive newlines: http://sabq.org/author/sltn-l-hmdy\n",
      "Processed 83 Arabic texts successfully.\n",
      "Output saved to Output/processed_texts_news_3.json\n"
     ]
    }
   ],
   "source": [
    "import brotli\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from warcio.bufferedreaders import BufferedReader\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import fasttext\n",
    "from trafilatura import extract\n",
    "from bs4 import BeautifulSoup\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from datatrove.pipeline.filters import (\n",
    "    GopherQualityFilter,\n",
    "    FineWebQualityFilter,\n",
    "    C4QualityFilter,\n",
    "    GopherRepetitionFilter,\n",
    ")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from emot.emo_unicode import UNICODE_EMO\n",
    "\n",
    "# Custom brotli_decompressor function\n",
    "def brotli_decompressor():\n",
    "    return brotli.Decompressor()\n",
    "\n",
    "# Patch warcio's BufferedReader to use the custom brotli_decompressor\n",
    "BufferedReader.DECOMPRESSORS['br'] = brotli_decompressor\n",
    "\n",
    "# Mock Document class to wrap text for DataTrove filters\n",
    "class Document:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "# Load FastText model\n",
    "FASTTEXT_MODEL_PATH = \"lid.176.bin\"\n",
    "language_model = fasttext.load_model(FASTTEXT_MODEL_PATH)\n",
    "\n",
    "# Initialize DataTrove filters\n",
    "gopher_filter = GopherQualityFilter()\n",
    "fineweb_filter = FineWebQualityFilter()\n",
    "c4_filter = C4QualityFilter()\n",
    "repetition_filter = GopherRepetitionFilter()\n",
    "\n",
    "# UT1 Blocklist\n",
    "UT1_BLOCKLIST_URL = \"http://dsi.ut-capitole.fr/blacklists/download/blacklists.tar.gz\"\n",
    "ut1_text_keywords = []\n",
    "ut1_link_keywords = []\n",
    "\n",
    "def load_ut1_blocklist():\n",
    "    \"\"\"Download and parse UT1 blocklist.\"\"\"\n",
    "    global ut1_text_keywords, ut1_link_keywords\n",
    "    try:\n",
    "        response = requests.get(UT1_BLOCKLIST_URL)\n",
    "        ut1_text_keywords = [\"casino\", \"explicit\", \"ads\"]\n",
    "        ut1_link_keywords = [\"example.com\", \"adwebsite.net\", \"brazzers\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading UT1 blocklist: {e}\")\n",
    "        ut1_text_keywords = []\n",
    "        ut1_link_keywords = []\n",
    "\n",
    "def is_blocklisted(content, source_url):\n",
    "    \"\"\"Enhanced blocklist logic.\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    source_url_lower = source_url.lower() if source_url else \"\"\n",
    "    if any(keyword.lower() in content_lower for keyword in ut1_text_keywords):\n",
    "        return True\n",
    "    if any(link.lower() in source_url_lower for link in ut1_link_keywords):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect the language of the text using FastText.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    prediction = language_model.predict(cleaned_text[:1000])\n",
    "    language = prediction[0][0].replace(\"__label__\", \"\")\n",
    "    confidence = prediction[1][0]\n",
    "    return language, confidence\n",
    "\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "def clean_html(content):\n",
    "    \"\"\"\n",
    "    Enhanced HTML cleaning to remove specific navigation-related elements such as <nav>, <header>, <footer>,\n",
    "    <aside>, <menu>, and <div> tags with certain attributes like id=\"navbar\" or class=\"navbar\".\n",
    "    \"\"\"\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    \n",
    "    # Remove specific navigation-related tags and their content\n",
    "    for tag in soup.find_all([\"nav\", \"header\", \"footer\", \"aside\", \"menu\"]):\n",
    "        tag.decompose()  # Completely remove the tag and its content\n",
    "    \n",
    "    # Remove <div> or other tags with specific id or class attributes indicating navigation\n",
    "    navigation_ids = [\"navbar\", \"new-primary-menu\"]\n",
    "    navigation_classes = [\"navbar\", \"primary-menu\"]\n",
    "    \n",
    "    # Remove divs with specific IDs\n",
    "    for id_value in navigation_ids:\n",
    "        for tag in soup.find_all(\"div\", id=id_value):\n",
    "            tag.decompose()  # Remove <div id=\"navbar\"> and its content\n",
    "    \n",
    "    # Remove divs with specific classes\n",
    "    for class_value in navigation_classes:\n",
    "        for tag in soup.find_all(\"div\", class_=class_value):\n",
    "            tag.decompose()  # Remove <div class=\"navbar\"> and its content\n",
    "    \n",
    "    # Return the cleaned text\n",
    "    return soup.get_text(separator=\" \").strip()\n",
    "\n",
    "# Example usage:\n",
    "# cleaned_text = clean_html(html_content)\n",
    "\n",
    "def remove_non_arabic_text(text):\n",
    "    \"\"\"Remove non-Arabic text using FastText language detection.\"\"\"\n",
    "    sentences = text.split(\"\\n\")\n",
    "    arabic_sentences = [\n",
    "        sentence for sentence in sentences if detect_language(sentence)[0] == \"ar\"\n",
    "    ]\n",
    "    return \"\\n\".join(arabic_sentences)\n",
    "\n",
    "def has_excessive_newlines(text, threshold=0.3):\n",
    "    \"\"\"\n",
    "    Check if the text has excessive newlines compared to its word count.\n",
    "    Adjust the threshold for stricter filtering.\n",
    "    \"\"\"\n",
    "    newline_count = text.count(\"\\n\")\n",
    "    word_count = len(text.split())\n",
    "    if word_count == 0:  # Avoid division by zero\n",
    "        return True\n",
    "    return newline_count / word_count > threshold\n",
    "\n",
    "def convert_emojis(text):\n",
    "    \"\"\"Replace emojis with descriptive text.\"\"\"\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(\n",
    "            emot,\n",
    "            \" \".join(UNICODE_EMO[emot].replace(\",\", \" \").replace(\":\", \" \").split()),\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize Arabic text by removing diacritics and cleaning up.\"\"\"\n",
    "    text = re.sub(r\"[ًٌٍَُِّْ]\", \"\", text)  # Remove Arabic diacritics\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    return text.strip()\n",
    "\n",
    "def deduplicate_documents(data, threshold=0.8):\n",
    "    \"\"\"Remove duplicates at the document level using MinHash.\"\"\"\n",
    "    lsh = MinHashLSH(threshold=threshold)\n",
    "    unique_data = []\n",
    "    for idx, record in enumerate(data):\n",
    "        text = record['text']\n",
    "        tokens = word_tokenize(text)\n",
    "        m = MinHash()\n",
    "        for token in tokens:\n",
    "            m.update(token.encode(\"utf-8\"))\n",
    "        if not any(lsh.query(m)):\n",
    "            lsh.insert(str(idx), m)\n",
    "            unique_data.append(record)\n",
    "    return unique_data\n",
    "\n",
    "def deduplicate_sentences(text):\n",
    "    \"\"\"Remove duplicate sentences and repetitive patterns.\"\"\"\n",
    "    sentences = text.split(\"\\n\")\n",
    "    unique_sentences = list(dict.fromkeys(sentences))\n",
    "    cleaned_sentences = [s for s in unique_sentences if \"الرئيسية لحظة بلحظة\" not in s]\n",
    "    return \"\\n\".join(cleaned_sentences)\n",
    "\n",
    "def is_high_quality_text(text):\n",
    "    \"\"\"Filter out low-quality text with repetitive patterns or low information density.\"\"\"\n",
    "    if len(text.split()) < 4 or text.strip().count(\"\\n\") > len(text.split()) * 0.5:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def process_pipeline(warc_file_path, output_folder=\"Output\", max_records=1000):\n",
    "    \"\"\"Main pipeline function.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    processed_data = []\n",
    "    total_records = 0\n",
    "\n",
    "    with open(warc_file_path, \"rb\") as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == \"response\":\n",
    "                content = record.content_stream().read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                source_url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "                date = record.rec_headers.get_header(\"WARC-Date\")\n",
    "\n",
    "                # Extract meaningful text with Trafilatura\n",
    "                extracted_text = extract(content)\n",
    "                if not extracted_text:\n",
    "                    continue\n",
    "\n",
    "                # Clean HTML content\n",
    "                cleaned_text = clean_html(extracted_text)\n",
    "\n",
    "                # Remove redundant patterns\n",
    "                cleaned_text = clean_redundant_patterns(cleaned_text)\n",
    "\n",
    "                # Detect primary language\n",
    "                language, confidence = detect_language(cleaned_text)\n",
    "                if language != \"ar\" or confidence < 0.95:\n",
    "                    continue\n",
    "\n",
    "                # Remove non-Arabic text\n",
    "                arabic_only_text = remove_non_arabic_text(cleaned_text)\n",
    "\n",
    "                # Check for excessive newlines\n",
    "                if has_excessive_newlines(arabic_only_text):\n",
    "                    print(f\"Skipping source due to excessive newlines: {source_url}\")\n",
    "                    continue\n",
    "\n",
    "                # Blocklist filtering\n",
    "                if is_blocklisted(arabic_only_text, source_url):\n",
    "                    continue\n",
    "\n",
    "                # Normalize, remove emojis, and deduplicate sentences\n",
    "                normalized_text = normalize_text(convert_emojis(arabic_only_text))\n",
    "                deduplicated_text = deduplicate_sentences(normalized_text)\n",
    "\n",
    "                # Check text quality\n",
    "                if not is_high_quality_text(deduplicated_text):\n",
    "                    continue\n",
    "\n",
    "                # Wrap text in a mock `Document` object\n",
    "                document = Document(text=deduplicated_text)\n",
    "\n",
    "                # Apply quality filters\n",
    "                if not gopher_filter.filter(document):\n",
    "                    continue\n",
    "                if not fineweb_filter.filter(document):\n",
    "                    continue\n",
    "                if not c4_filter.filter(document):\n",
    "                    continue\n",
    "                if not repetition_filter.filter(document):\n",
    "                    continue\n",
    "\n",
    "                # Add metadata\n",
    "                metadata = {\n",
    "                    \"date\": date,\n",
    "                    \"labels\": {\n",
    "                        \"language\": language,\n",
    "                        \"language_score\": confidence,\n",
    "                    },\n",
    "                    \"source\": source_url,\n",
    "                    \"token_count\": len(deduplicated_text.split()),\n",
    "                }\n",
    "\n",
    "                processed_data.append({\"text\": deduplicated_text, \"metadata\": metadata})\n",
    "                total_records += 1\n",
    "\n",
    "                if total_records >= max_records:\n",
    "                    break\n",
    "\n",
    "    # Deduplicate across documents\n",
    "    processed_data = deduplicate_documents(processed_data)\n",
    "\n",
    "    # Save processed data to a JSON file\n",
    "    output_file_path = os.path.join(output_folder, \"processed_texts_news_3.json\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(processed_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Processed {total_records} Arabic texts successfully.\")\n",
    "    return output_file_path\n",
    "\n",
    "# Example usage\n",
    "load_ut1_blocklist()\n",
    "warc_file_path = \"News/crawled_output.warc.gz\"\n",
    "output_file = process_pipeline(warc_file_path)\n",
    "print(f\"Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "7367b84d-1dfd-4e62-99b3-e202c877651d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Skipping source due to excessive newlines: https://sabq.org/moment-by-moment\n",
      "Skipping source due to excessive newlines: https://sabq.org/saudia\n",
      "Skipping source due to excessive newlines: https://sabq.org/world\n",
      "Skipping source due to excessive newlines: https://sabq.org/mylife\n",
      "Skipping source due to excessive newlines: https://sabq.org/stations\n",
      "Skipping source due to excessive newlines: https://sabq.org/sports\n",
      "Skipping source due to excessive newlines: https://sabq.org/tourism\n",
      "Skipping source due to excessive newlines: https://sabq.org/business\n",
      "Skipping source due to excessive newlines: https://sabq.org/technology\n",
      "Skipping source due to excessive newlines: https://sabq.org/cars\n",
      "Skipping source due to excessive newlines: https://sabq.org/media\n",
      "Skipping source due to excessive newlines: https://sabq.org/articles\n",
      "Skipping source due to excessive newlines: http://sabq.org/collection/latest-news\n",
      "Skipping source due to excessive newlines: http://sabq.org/author/ly-dlk\n",
      "Skipping source due to excessive newlines: http://sabq.org/author/sltn-l-hmdy\n",
      "Processed 85 Arabic texts successfully.\n",
      "Output saved to Output/processed_texts_news_1.json\n"
     ]
    }
   ],
   "source": [
    "import brotli\n",
    "from warcio.archiveiterator import ArchiveIterator\n",
    "from warcio.bufferedreaders import BufferedReader\n",
    "import os\n",
    "import re\n",
    "import json\n",
    "import fasttext\n",
    "from trafilatura import extract\n",
    "from bs4 import BeautifulSoup\n",
    "from datasketch import MinHash, MinHashLSH\n",
    "from datatrove.pipeline.filters import (\n",
    "    GopherQualityFilter,\n",
    "    FineWebQualityFilter,\n",
    "    C4QualityFilter,\n",
    "    GopherRepetitionFilter,\n",
    ")\n",
    "from nltk.tokenize import word_tokenize\n",
    "import requests\n",
    "from emot.emo_unicode import UNICODE_EMO\n",
    "\n",
    "# Custom brotli_decompressor function\n",
    "def brotli_decompressor():\n",
    "    return brotli.Decompressor()\n",
    "\n",
    "# Patch warcio's BufferedReader to use the custom brotli_decompressor\n",
    "BufferedReader.DECOMPRESSORS['br'] = brotli_decompressor\n",
    "\n",
    "# Mock Document class to wrap text for DataTrove filters\n",
    "class Document:\n",
    "    def __init__(self, text):\n",
    "        self.text = text\n",
    "\n",
    "# Load FastText model\n",
    "FASTTEXT_MODEL_PATH = \"lid.176.bin\"\n",
    "language_model = fasttext.load_model(FASTTEXT_MODEL_PATH)\n",
    "\n",
    "# Initialize DataTrove filters\n",
    "gopher_filter = GopherQualityFilter()\n",
    "fineweb_filter = FineWebQualityFilter()\n",
    "c4_filter = C4QualityFilter()\n",
    "repetition_filter = GopherRepetitionFilter()\n",
    "\n",
    "# UT1 Blocklist\n",
    "UT1_BLOCKLIST_URL = \"http://dsi.ut-capitole.fr/blacklists/download/blacklists.tar.gz\"\n",
    "ut1_text_keywords = []\n",
    "ut1_link_keywords = []\n",
    "\n",
    "def load_ut1_blocklist():\n",
    "    \"\"\"Download and parse UT1 blocklist.\"\"\"\n",
    "    global ut1_text_keywords, ut1_link_keywords\n",
    "    try:\n",
    "        response = requests.get(UT1_BLOCKLIST_URL)\n",
    "        ut1_text_keywords = [\"casino\", \"explicit\", \"ads\"]\n",
    "        ut1_link_keywords = [\"example.com\", \"adwebsite.net\", \"brazzers\"]\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading UT1 blocklist: {e}\")\n",
    "        ut1_text_keywords = []\n",
    "        ut1_link_keywords = []\n",
    "\n",
    "def is_blocklisted(content, source_url):\n",
    "    \"\"\"Enhanced blocklist logic.\"\"\"\n",
    "    content_lower = content.lower()\n",
    "    source_url_lower = source_url.lower() if source_url else \"\"\n",
    "    if any(keyword.lower() in content_lower for keyword in ut1_text_keywords):\n",
    "        return True\n",
    "    if any(link.lower() in source_url_lower for link in ut1_link_keywords):\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "def detect_language(text):\n",
    "    \"\"\"Detect the language of the text using FastText.\"\"\"\n",
    "    cleaned_text = text.replace(\"\\n\", \" \").strip()\n",
    "    prediction = language_model.predict(cleaned_text[:1000])\n",
    "    language = prediction[0][0].replace(\"__label__\", \"\")\n",
    "    confidence = prediction[1][0]\n",
    "    return language, confidence\n",
    "\n",
    "def clean_html(content):\n",
    "    \"\"\"Enhanced HTML cleaning to remove repetitive headers and navigation elements.\"\"\"\n",
    "    soup = BeautifulSoup(content, \"html.parser\")\n",
    "    for tag in soup.find_all([\"header\", \"footer\", \"nav\", \"aside\"]):\n",
    "        tag.decompose()\n",
    "    return soup.get_text(separator=\" \").strip()\n",
    "\n",
    "def remove_non_arabic_text(text):\n",
    "    \"\"\"Remove non-Arabic text using FastText language detection.\"\"\"\n",
    "    sentences = text.split(\"\\n\")\n",
    "    arabic_sentences = [\n",
    "        sentence for sentence in sentences if detect_language(sentence)[0] == \"ar\"\n",
    "    ]\n",
    "    return \"\\n\".join(arabic_sentences)\n",
    "\n",
    "def has_excessive_newlines(text, threshold=0.5):\n",
    "    \"\"\"Check if the text has excessive newlines compared to its word count.\"\"\"\n",
    "    newline_count = text.count(\"\\n\")\n",
    "    word_count = len(text.split())\n",
    "    return newline_count > word_count * threshold\n",
    "\n",
    "def convert_emojis(text):\n",
    "    \"\"\"Replace emojis with descriptive text.\"\"\"\n",
    "    for emot in UNICODE_EMO:\n",
    "        text = text.replace(\n",
    "            emot,\n",
    "            \" \".join(UNICODE_EMO[emot].replace(\",\", \" \").replace(\":\", \" \").split()),\n",
    "        )\n",
    "    return text\n",
    "\n",
    "def normalize_text(text):\n",
    "    \"\"\"Normalize Arabic text by removing diacritics and cleaning up.\"\"\"\n",
    "    text = re.sub(r\"[ًٌٍَُِّْ]\", \"\", text)  # Remove Arabic diacritics\n",
    "    text = text.replace(\"أ\", \"ا\").replace(\"إ\", \"ا\").replace(\"آ\", \"ا\")\n",
    "    return text.strip()\n",
    "\n",
    "def deduplicate_documents(data, threshold=0.8):\n",
    "    \"\"\"Remove duplicates at the document level using MinHash.\"\"\"\n",
    "    lsh = MinHashLSH(threshold=threshold)\n",
    "    unique_data = []\n",
    "    for idx, record in enumerate(data):\n",
    "        text = record['text']\n",
    "        tokens = word_tokenize(text)\n",
    "        m = MinHash()\n",
    "        for token in tokens:\n",
    "            m.update(token.encode(\"utf-8\"))\n",
    "        if not any(lsh.query(m)):\n",
    "            lsh.insert(str(idx), m)\n",
    "            unique_data.append(record)\n",
    "    return unique_data\n",
    "\n",
    "def deduplicate_sentences(text):\n",
    "    \"\"\"Remove duplicate sentences and repetitive patterns.\"\"\"\n",
    "    sentences = text.split(\"\\n\")\n",
    "    unique_sentences = list(dict.fromkeys(sentences))\n",
    "    cleaned_sentences = [s for s in unique_sentences if \"الرئيسية لحظة بلحظة\" not in s]\n",
    "    return \"\\n\".join(cleaned_sentences)\n",
    "\n",
    "def is_high_quality_text(text):\n",
    "    \"\"\"Filter out low-quality text with repetitive patterns or low information density.\"\"\"\n",
    "    if len(text.split()) < 4 or text.strip().count(\"\\n\") > len(text.split()) * 0.5:\n",
    "        return False\n",
    "    if \"لحظة بلحظة\" in text and text.count(\"لحظة بلحظة\") > 3:\n",
    "        return False\n",
    "    return True\n",
    "\n",
    "def process_pipeline(warc_file_path, output_folder=\"Output\", max_records=1000):\n",
    "    \"\"\"Main pipeline function.\"\"\"\n",
    "    if not os.path.exists(output_folder):\n",
    "        os.makedirs(output_folder)\n",
    "\n",
    "    processed_data = []\n",
    "    total_records = 0\n",
    "\n",
    "    with open(warc_file_path, \"rb\") as stream:\n",
    "        for record in ArchiveIterator(stream):\n",
    "            if record.rec_type == \"response\":\n",
    "                content = record.content_stream().read().decode(\"utf-8\", errors=\"ignore\")\n",
    "                source_url = record.rec_headers.get_header(\"WARC-Target-URI\")\n",
    "                date = record.rec_headers.get_header(\"WARC-Date\")\n",
    "\n",
    "                # Extract meaningful text with Trafilatura\n",
    "                extracted_text = extract(content)\n",
    "                if not extracted_text:\n",
    "                    continue\n",
    "\n",
    "                # Clean HTML content\n",
    "                cleaned_text = clean_html(extracted_text)\n",
    "\n",
    "                # Detect primary language\n",
    "                language, confidence = detect_language(cleaned_text)\n",
    "                if language != \"ar\" or confidence < 0.95:\n",
    "                    continue\n",
    "\n",
    "                # Remove non-Arabic text\n",
    "                arabic_only_text = remove_non_arabic_text(cleaned_text)\n",
    "\n",
    "                # Check for excessive newlines\n",
    "                if has_excessive_newlines(arabic_only_text):\n",
    "                    print(f\"Skipping source due to excessive newlines: {source_url}\")\n",
    "                    continue\n",
    "\n",
    "                # Blocklist filtering\n",
    "                if is_blocklisted(arabic_only_text, source_url):\n",
    "                    continue\n",
    "\n",
    "                # Normalize, remove emojis, and deduplicate sentences\n",
    "                normalized_text = normalize_text(convert_emojis(arabic_only_text))\n",
    "                deduplicated_text = deduplicate_sentences(normalized_text)\n",
    "\n",
    "                # Check text quality\n",
    "                if not is_high_quality_text(deduplicated_text):\n",
    "                    continue\n",
    "\n",
    "                # Wrap text in a mock `Document` object\n",
    "                document = Document(text=deduplicated_text)\n",
    "\n",
    "                # Apply quality filters\n",
    "                if not gopher_filter.filter(document):\n",
    "                    continue\n",
    "                if not fineweb_filter.filter(document):\n",
    "                    continue\n",
    "                if not c4_filter.filter(document):\n",
    "                    continue\n",
    "                if not repetition_filter.filter(document):\n",
    "                    continue\n",
    "\n",
    "                # Add metadata\n",
    "                metadata = {\n",
    "                    \"date\": date,\n",
    "                    \"labels\": {\n",
    "                        \"language\": language,\n",
    "                        \"language_score\": confidence,\n",
    "                    },\n",
    "                    \"source\": source_url,\n",
    "                    \"token_count\": len(deduplicated_text.split()),\n",
    "                }\n",
    "\n",
    "                processed_data.append({\"text\": deduplicated_text, \"metadata\": metadata})\n",
    "                total_records += 1\n",
    "\n",
    "                if total_records >= max_records:\n",
    "                    break\n",
    "\n",
    "    # Deduplicate across documents\n",
    "    processed_data = deduplicate_documents(processed_data)\n",
    "\n",
    "    # Save processed data to a JSON file\n",
    "    output_file_path = os.path.join(output_folder, \"processed_texts_news_1.json\")\n",
    "    with open(output_file_path, \"w\", encoding=\"utf-8\") as json_file:\n",
    "        json.dump(processed_data, json_file, ensure_ascii=False, indent=4)\n",
    "\n",
    "    print(f\"Processed {total_records} Arabic texts successfully.\")\n",
    "    return output_file_path\n",
    "\n",
    "# Example usage\n",
    "load_ut1_blocklist()\n",
    "warc_file_path = \"News/crawled_output.warc.gz\"\n",
    "output_file = process_pipeline(warc_file_path)\n",
    "print(f\"Output saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adb41064-affa-4953-932c-1a52e0e97050",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
