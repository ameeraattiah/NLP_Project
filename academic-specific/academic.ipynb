{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "8639ba53-5532-42a4-b915-8f0bcc6f570e",
   "metadata": {},
   "source": [
    "# Academic Specific Code"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6515cfe8-5b18-47cc-9c68-afda621eeb48",
   "metadata": {},
   "source": [
    "## Install Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b2a68a2-067b-452a-9018-86d08c09d863",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install transformers torch\n",
    "!pip install transformers torch scikit-learn\n",
    "!pip install ipywidgets --upgrade\n",
    "!pip install datasets --upgrade\n",
    "!pip install pyarrow --upgrade\n",
    "!pip install huggingface_hub\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install datasets\n",
    "!pip install datasketch\n",
    "!pip install transformers[torch] accelerate\n",
    "!pip install ipywidgets\n",
    "!pip install ipywidgets\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "!pip install requests\n",
    "!pip install tiktoken\n",
    "!pip install sentencepiece\n",
    "!pip install --upgrade notebook ipywidgets\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04db7996-8ec7-42b1-ac47-d29c6fd3c556",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "357ee16d-6cad-428c-92ed-f1fa7528d1b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()\n",
    "\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "\n",
    "print(\"SentencePiece is installed and ready to use.\")\n",
    "\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "951e15c5-e9c9-4eef-926a-18a8ded32897",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3935c554-60fa-4163-8894-fff8fadfa5f8",
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"model_name\": \"UBC-NLP/araT5-base\",  # Load AraT5 locally\n",
    "    \"fine_tune_model\": \"aubmindlab/bert-base-arabertv02\",  # Model to fine-tune\n",
    "    \"threshold\": 3,  # Minimum score for high-quality content\n",
    "    \"annotation_samples\": 100,  # Total number of samples annotated with AraT5.\n",
    "    \"validation_samples\": 30,  # Subset of annotation_samples reserved for validation\n",
    "    \"max_samples_to_fine_tune\": 70,  # Maximum annotated samples used for fine-tuning\n",
    "    \"epochs\": 5,  # Fine-tuning epochs\n",
    "    \"batch_size\": 4,  # Lower the batch size to reduce memory usage\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "513f135a-83a1-44d6-a7a8-fe219f2ce729",
   "metadata": {},
   "source": [
    "## Arabic Rubric Prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db3d9c22-b051-42bb-84f8-c9b5b181c731",
   "metadata": {},
   "outputs": [],
   "source": [
    "arabic_prompt = \"\"\"\n",
    "فيما يلي مقتطف من صفحة ويب. قم بتقييم ما إذا كانت الصفحة ذات قيمة تعليمية عالية ويمكن أن تكون مفيدة في بيئة تعليمية لتدريس المستويات من المرحلة الابتدائية إلى المرحلة الإعدادية باستخدام نظام تقييم مكون من 5 نقاط تراكمية وفقًا للمعايير التالية:\n",
    "أضف نقطة واحدة إذا كان المقتطف يقدم بعض المعلومات الأساسية ذات الصلة بالموضوعات التعليمية، حتى لو تضمن محتوى غير ذي صلة أو غير أكاديمي مثل الإعلانات والمواد الترويجية.\n",
    "•\tأضف نقطة أخرى إذا كان المقتطف يتناول بعض العناصر ذات الصلة بالتعليم ولكنه لا يتماشى بشكل وثيق مع المعايير التعليمية. قد يخلط بين المحتوى التعليمي وغير التعليمي، ويقدم نظرة عامة سطحية عن موضوعات قد تكون مفيدة، أو يعرض المعلومات بطريقة غير منظمة وأسلوب كتابة غير واضح.\n",
    "•\tامنح نقطة ثالثة إذا كان المقتطف مناسبًا للاستخدام التعليمي ويقدم مفاهيم رئيسية ذات صلة بالمناهج المدرسية. يكون المحتوى واضحًا ولكنه قد لا يكون شاملاً، أو قد يتضمن بعض المعلومات الزائدة. قد يشبه القسم التمهيدي لكتاب مدرسي أو درس تعليمي بسيط مناسب للتعلم ولكنه يحتوي على بعض القيود مثل معالجة مفاهيم معقدة جدًا لطلاب المرحلة الإعدادية.\n",
    "•\tامنح نقطة رابعة إذا كان المقتطف ذا صلة كبيرة ومفيدًا للأغراض التعليمية لمستوى لا يتجاوز المرحلة الإعدادية، مع أسلوب كتابة واضح ومتسق. يمكن أن يشبه فصلًا من كتاب مدرسي أو درسًا تعليميًا، حيث يقدم محتوى تعليميًا غنيًا، بما في ذلك التمارين والحلول، مع الحد الأدنى من المعلومات غير ذات الصلة، والمفاهيم ليست معقدة للغاية لطلاب هذه المرحلة. يكون المحتوى منظمًا ومركّزًا وقيمًا للتعلم المنهجي.\n",
    "•\tامنح نقطة خامسة إذا كان المقتطف ممتازًا في قيمته التعليمية ومناسبًا تمامًا للتدريس في المرحلة الابتدائية أو الإعدادية. يتبع المقتطف منطقًا تفصيليًا، وأسلوب الكتابة سهل الفهم، ويقدم رؤى عميقة وشاملة حول الموضوع دون أي محتوى غير تعليمي أو معقد.\n",
    "المقتطف: <EXAMPLE>. بعد فحص المقتطف:\n",
    "•\tبرر بإيجاز مجموع النقاط، بحد أقصى 100 كلمة.\n",
    "•\tاختتم بالنقاط الإجمالية بالتنسيق التالي: \"التقييم التعليمي: <مجموع النقاط>\".\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7928c3ef-61cd-41d7-a3ed-895a08efcba1",
   "metadata": {},
   "source": [
    "## Custom Dataset Class\n",
    "standardizes the process of preparing text data for machine learning models by tokenizing text, truncating or padding sequences to a fixed length, and formatting inputs and labels into PyTorch tensors. This enables efficient batching and compatibility with PyTorch's DataLoader for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88dd12c3-8d40-4f1d-ad63-6a7a789f265a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return item"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5af5c91b-94c1-4315-aad9-7495da630f3f",
   "metadata": {},
   "source": [
    "## Step 1: Load Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4f02582-a699-4717-a332-b663363d53a5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    return [{\"text\": item[\"text\"], \"metadata\": item[\"metadata\"]} for item in data]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb37df89-e9a9-43a8-9889-1319e8642432",
   "metadata": {},
   "source": [
    "## Step 2: Annotate Data Locally with AraT5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4864b730-1edf-4370-a812-e9369423cbd1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def annotate_samples(samples, model_name):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    annotated_data = []\n",
    "\n",
    "    for sample in samples:\n",
    "        text = sample[\"text\"]\n",
    "        prompt = arabic_prompt.format(text=text)\n",
    "\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = model.generate(**inputs, max_length=100)\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        scores = extract_scores(result)\n",
    "        annotated_data.append({\"text\": text, \"scores\": scores, \"metadata\": sample[\"metadata\"]})\n",
    "\n",
    "    return annotated_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c587b41-8e9e-4e21-8a9f-6e8933cd52f0",
   "metadata": {},
   "source": [
    "## Extract Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f79a22-b98b-45a9-9535-7617314ae1f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_scores(output):\n",
    "    lines = output.split(\"\\n\")\n",
    "    scores = {}\n",
    "    for line in lines:\n",
    "        if \"ملاءمة\" in line:\n",
    "            scores[\"relevance\"] = int(line.split(\":\")[-1].strip())\n",
    "        elif \"وضوح\" in line:\n",
    "            scores[\"clarity\"] = int(line.split(\":\")[-1].strip())\n",
    "        elif \"عمق\" in line:\n",
    "            scores[\"depth\"] = int(line.split(\":\")[-1].strip())\n",
    "    total = sum(scores.values())\n",
    "    scores[\"total\"] = total\n",
    "    return scores"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddda99a7-f9e0-4f84-bb02-ef0e7b1d0f24",
   "metadata": {},
   "source": [
    "## Step 3: Fine-Tune AraBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "febb1e7a-35f7-4d95-b2d8-d8ab8c0b9477",
   "metadata": {},
   "outputs": [],
   "source": [
    "def fine_tune_arabert(train_data, tokenizer, model):\n",
    "    texts = [item[\"text\"] for item in train_data]\n",
    "    labels = [item[\"scores\"][\"total\"] for item in train_data]\n",
    "\n",
    "    dataset = CustomDataset(texts, labels, tokenizer)\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "        output_dir=\"./results\",\n",
    "        num_train_epochs=config[\"epochs\"],\n",
    "        per_device_train_batch_size=config[\"batch_size\"],\n",
    "        save_steps=10_000,\n",
    "        save_total_limit=2,\n",
    "        no_cuda=True,\n",
    "        logging_dir=\"./logs\",\n",
    "        logging_steps=100,\n",
    "    )\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db0d915a-d6a1-4aaf-bb9c-b3a76deff20e",
   "metadata": {},
   "source": [
    "## Step 4: Predict with Fine-Tuned AraBERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68fd8f94-37a4-4ec1-9d29-79fc68eba96b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_with_arabert(unlabeled_data, model, tokenizer):\n",
    "    model.eval()\n",
    "    model.to(device)\n",
    "    predictions = []\n",
    "\n",
    "    for sample in unlabeled_data:\n",
    "        text = sample[\"text\"]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = logits.argmax(dim=-1).item()\n",
    "\n",
    "        predictions.append({\"text\": text, \"predicted_score\": predicted_label, \"metadata\": sample[\"metadata\"]})\n",
    "\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c6489a8-dac1-4ca9-b3c4-afe3b8ca4f83",
   "metadata": {},
   "source": [
    "## Step 5: Validate Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28462c88-9d9b-47ef-b446-f3496dcabc8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate_model(validation_data, model, tokenizer):\n",
    "    model.to(device)\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for item in validation_data:\n",
    "        text = item[\"text\"]\n",
    "        true_labels.append(item[\"scores\"][\"total\"])\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = logits.argmax(dim=-1).item()\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(true_labels, predicted_labels, average=\"macro\", zero_division=0)\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    print(f\"Validation F1 Score: {f1:.2f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Validation Precision: {precision:.2f}\")\n",
    "    print(f\"Validation Recall: {recall:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b078a2d6-bdc4-4ff1-b358-979d60a1fc8d",
   "metadata": {},
   "source": [
    "## Step 6: Filter Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9633d611-7889-40ef-991d-3371db43a9a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_dataset(annotated_data, threshold):\n",
    "    return [\n",
    "        doc for doc in annotated_data\n",
    "        if doc[\"predicted_score\"] >= threshold\n",
    "    ]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a4a873f-f5cf-46c4-8abf-126e2e5cab83",
   "metadata": {},
   "source": [
    "## Main Pipeline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02a75351-5088-4a12-afa7-1aed2643469e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def main_pipeline():\n",
    "    # Step 1: Load the dataset from a specified JSON file\n",
    "    dataset = load_dataset(\"/Users/ameeraattiah/Desktop/arabicweb24/jeje.json\")\n",
    "    print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "    # Step 2: Select a subset of the dataset for annotation\n",
    "    sample_data = dataset[:config[\"annotation_samples\"]]\n",
    "    annotated_data = annotate_samples(sample_data, config[\"model_name\"])\n",
    "    print(f\"Annotated {len(annotated_data)} samples.\")\n",
    "\n",
    "    # Step 3: Load the tokenizer and model for fine-tuning\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"fine_tune_model\"])\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(config[\"fine_tune_model\"], num_labels=6)\n",
    "\n",
    "    # Step 4: Fine-tune the model using the annotated data\n",
    "    fine_tune_arabert(annotated_data, tokenizer, model)\n",
    "\n",
    "    # Step 5: Use the fine-tuned model to predict the remaining dataset\n",
    "    remaining_data = dataset[config[\"annotation_samples\"]:]\n",
    "    predictions = predict_with_arabert(remaining_data, model, tokenizer)\n",
    "    print(f\"Predicted {len(predictions)} samples with fine-tuned AraBERT.\")\n",
    "\n",
    "    # Step 6: Validate the fine-tuned model on a subset of the annotated data\n",
    "    validation_data = annotated_data[:config[\"validation_samples\"]]\n",
    "    validate_model(validation_data, model, tokenizer)\n",
    "\n",
    "    # Step 7: Filter the predictions to include only high-quality samples\n",
    "    filtered_data = filter_dataset(predictions, config[\"threshold\"])\n",
    "    print(f\"Filtered dataset contains {len(filtered_data)} high-quality samples.\")\n",
    "\n",
    "    # Step 8: Save the filtered data to a JSON file for future use\n",
    "    with open(\"/Users/ameeraattiah/Desktop/arabicweb24/jeje-edu.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(filtered_data, file, ensure_ascii=False, indent=4)\n",
    "    print(\"Filtered data saved.\")\n",
    "\n",
    "main_pipeline()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "616444cd-7358-4ec9-82ff-52bbb72fe09e",
   "metadata": {},
   "source": [
    "## Original Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "76ebe655-8a0a-48bb-a3f8-cebc91e49532",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.46.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.46.1)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: pyarrow in /opt/anaconda3/lib/python3.12/site-packages (18.0.0)\n",
      "Requirement already satisfied: huggingface_hub in /opt/anaconda3/lib/python3.12/site-packages (0.26.2)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (3.13.1)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (2024.3.1)\n",
      "Requirement already satisfied: packaging>=20.9 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (6.0.1)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface_hub) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->huggingface_hub) (2024.8.30)\n",
      "Requirement already satisfied: torch in /opt/anaconda3/lib/python3.12/site-packages (2.5.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.13.1)\n",
      "Requirement already satisfied: typing-extensions>=4.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (4.11.0)\n",
      "Requirement already satisfied: networkx in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.2.1)\n",
      "Requirement already satisfied: jinja2 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (3.1.4)\n",
      "Requirement already satisfied: fsspec in /opt/anaconda3/lib/python3.12/site-packages (from torch) (2024.3.1)\n",
      "Requirement already satisfied: setuptools in /opt/anaconda3/lib/python3.12/site-packages (from torch) (69.5.1)\n",
      "Requirement already satisfied: sympy==1.13.1 in /opt/anaconda3/lib/python3.12/site-packages (from torch) (1.13.1)\n",
      "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from sympy==1.13.1->torch) (1.3.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2->torch) (2.1.3)\n",
      "Requirement already satisfied: transformers in /opt/anaconda3/lib/python3.12/site-packages (4.46.1)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (3.13.1)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.26.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2023.10.3)\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (2.32.2)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.4.5)\n",
      "Requirement already satisfied: tokenizers<0.21,>=0.20 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (0.20.2)\n",
      "Requirement already satisfied: tqdm>=4.27 in /opt/anaconda3/lib/python3.12/site-packages (from transformers) (4.66.4)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests->transformers) (2024.8.30)\n",
      "Requirement already satisfied: scikit-learn in /opt/anaconda3/lib/python3.12/site-packages (1.4.2)\n",
      "Requirement already satisfied: numpy>=1.19.5 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.13.1)\n",
      "Requirement already satisfied: joblib>=1.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: threadpoolctl>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from scikit-learn) (2.2.0)\n",
      "Requirement already satisfied: datasets in /opt/anaconda3/lib/python3.12/site-packages (3.1.0)\n",
      "Requirement already satisfied: filelock in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.13.1)\n",
      "Requirement already satisfied: numpy>=1.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (1.26.4)\n",
      "Requirement already satisfied: pyarrow>=15.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (18.0.0)\n",
      "Requirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.3.8)\n",
      "Requirement already satisfied: pandas in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.2.2)\n",
      "Requirement already satisfied: requests>=2.32.2 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (2.32.2)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (4.66.4)\n",
      "Requirement already satisfied: xxhash in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.5.0)\n",
      "Requirement already satisfied: multiprocess<0.70.17 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.70.16)\n",
      "Requirement already satisfied: fsspec<=2024.9.0,>=2023.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from fsspec[http]<=2024.9.0,>=2023.1.0->datasets) (2024.3.1)\n",
      "Requirement already satisfied: aiohttp in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (3.9.5)\n",
      "Requirement already satisfied: huggingface-hub>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (0.26.2)\n",
      "Requirement already satisfied: packaging in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (23.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /opt/anaconda3/lib/python3.12/site-packages (from datasets) (6.0.1)\n",
      "Requirement already satisfied: aiosignal>=1.1.2 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.2.0)\n",
      "Requirement already satisfied: attrs>=17.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (23.1.0)\n",
      "Requirement already satisfied: frozenlist>=1.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.4.0)\n",
      "Requirement already satisfied: multidict<7.0,>=4.5 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (6.0.4)\n",
      "Requirement already satisfied: yarl<2.0,>=1.0 in /opt/anaconda3/lib/python3.12/site-packages (from aiohttp->datasets) (1.9.3)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /opt/anaconda3/lib/python3.12/site-packages (from huggingface-hub>=0.23.0->datasets) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.32.2->datasets) (2024.8.30)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2024.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/anaconda3/lib/python3.12/site-packages (from pandas->datasets) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in /opt/anaconda3/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\n",
      "Requirement already satisfied: datasketch in /opt/anaconda3/lib/python3.12/site-packages (1.6.5)\n",
      "Requirement already satisfied: numpy>=1.11 in /opt/anaconda3/lib/python3.12/site-packages (from datasketch) (1.26.4)\n",
      "Requirement already satisfied: scipy>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from datasketch) (1.13.1)\n",
      "zsh:1: no matches found: transformers[torch]\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six in /opt/anaconda3/lib/python3.12/site-packages (from asttokens->stack-data->ipython>=6.1.0->ipywidgets) (1.16.0)\n",
      "\u001b[33m(Deprecated) Installing extensions with the jupyter labextension install command is now deprecated and will be removed in a future major version of JupyterLab.\n",
      "\n",
      "Users should manage prebuilt extensions with package managers like pip and conda, and extension authors are encouraged to distribute their extensions as prebuilt packages \u001b[0m\n",
      "/opt/anaconda3/lib/python3.12/site-packages/jupyterlab/debuglog.py:54: UserWarning: An error occurred.\n",
      "  warnings.warn(\"An error occurred.\")\n",
      "/opt/anaconda3/lib/python3.12/site-packages/jupyterlab/debuglog.py:55: UserWarning: ValueError: Please install Node.js and npm before continuing installation. You may be able to install Node.js from your package manager, from conda, or directly from the Node.js website (https://nodejs.org).\n",
      "  warnings.warn(msg[-1].strip())\n",
      "/opt/anaconda3/lib/python3.12/site-packages/jupyterlab/debuglog.py:56: UserWarning: See the log file for details: /var/folders/hh/v5mfdbdn43z0n2v1mbb_yc_40000gn/T/jupyterlab-debug-y0zy4_oc.log\n",
      "  warnings.warn(f\"See the log file for details: {log_path!s}\")\n",
      "Requirement already satisfied: requests in /opt/anaconda3/lib/python3.12/site-packages (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests) (2024.8.30)\n",
      "Requirement already satisfied: tiktoken in /opt/anaconda3/lib/python3.12/site-packages (0.8.0)\n",
      "Requirement already satisfied: regex>=2022.1.18 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2023.10.3)\n",
      "Requirement already satisfied: requests>=2.26.0 in /opt/anaconda3/lib/python3.12/site-packages (from tiktoken) (2.32.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2.2.2)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.26.0->tiktoken) (2024.8.30)\n",
      "Requirement already satisfied: sentencepiece in /opt/anaconda3/lib/python3.12/site-packages (0.2.0)\n",
      "Requirement already satisfied: notebook in /opt/anaconda3/lib/python3.12/site-packages (7.2.2)\n",
      "Requirement already satisfied: ipywidgets in /opt/anaconda3/lib/python3.12/site-packages (8.1.5)\n",
      "Requirement already satisfied: jupyter-server<3,>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from notebook) (2.14.1)\n",
      "Requirement already satisfied: jupyterlab-server<3,>=2.27.1 in /opt/anaconda3/lib/python3.12/site-packages (from notebook) (2.27.3)\n",
      "Requirement already satisfied: jupyterlab<4.3,>=4.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from notebook) (4.2.6)\n",
      "Requirement already satisfied: notebook-shim<0.3,>=0.2 in /opt/anaconda3/lib/python3.12/site-packages (from notebook) (0.2.3)\n",
      "Requirement already satisfied: tornado>=6.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from notebook) (6.4.1)\n",
      "Requirement already satisfied: comm>=0.1.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (0.2.1)\n",
      "Requirement already satisfied: ipython>=6.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (8.25.0)\n",
      "Requirement already satisfied: traitlets>=4.3.1 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (5.14.3)\n",
      "Requirement already satisfied: widgetsnbextension~=4.0.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (4.0.13)\n",
      "Requirement already satisfied: jupyterlab-widgets~=3.0.12 in /opt/anaconda3/lib/python3.12/site-packages (from ipywidgets) (3.0.13)\n",
      "Requirement already satisfied: decorator in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.18.1)\n",
      "Requirement already satisfied: matplotlib-inline in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (2.15.1)\n",
      "Requirement already satisfied: stack-data in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (0.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /opt/anaconda3/lib/python3.12/site-packages (from ipython>=6.1.0->ipywidgets) (4.8.0)\n",
      "Requirement already satisfied: anyio>=3.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (4.2.0)\n",
      "Requirement already satisfied: argon2-cffi>=21.1 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (21.3.0)\n",
      "Requirement already satisfied: jinja2>=3.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (3.1.4)\n",
      "Requirement already satisfied: jupyter-client>=7.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (8.6.0)\n",
      "Requirement already satisfied: jupyter-core!=5.0.*,>=4.12 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.7.2)\n",
      "Requirement already satisfied: jupyter-events>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.10.0)\n",
      "Requirement already satisfied: jupyter-server-terminals>=0.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.4.4)\n",
      "Requirement already satisfied: nbconvert>=6.4.4 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.10.0)\n",
      "Requirement already satisfied: nbformat>=5.3.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (5.9.2)\n",
      "Requirement already satisfied: overrides>=5.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (7.4.0)\n",
      "Requirement already satisfied: packaging>=22.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (23.2)\n",
      "Requirement already satisfied: prometheus-client>=0.9 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.14.1)\n",
      "Requirement already satisfied: pyzmq>=24 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (25.1.2)\n",
      "Requirement already satisfied: send2trash>=1.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.2)\n",
      "Requirement already satisfied: terminado>=0.8.3 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (0.17.1)\n",
      "Requirement already satisfied: websocket-client>=1.7 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-server<3,>=2.4.0->notebook) (1.8.0)\n",
      "Requirement already satisfied: async-lru>=1.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (2.0.4)\n",
      "Requirement already satisfied: httpx>=0.25.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (0.27.0)\n",
      "Requirement already satisfied: ipykernel>=6.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (6.28.0)\n",
      "Requirement already satisfied: jupyter-lsp>=2.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (2.2.0)\n",
      "Requirement already satisfied: setuptools>=40.1.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab<4.3,>=4.2.0->notebook) (69.5.1)\n",
      "Requirement already satisfied: babel>=2.10 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.11.0)\n",
      "Requirement already satisfied: json5>=0.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (0.9.6)\n",
      "Requirement already satisfied: jsonschema>=4.18.0 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (4.19.2)\n",
      "Requirement already satisfied: requests>=2.31 in /opt/anaconda3/lib/python3.12/site-packages (from jupyterlab-server<3,>=2.27.1->notebook) (2.32.2)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (3.7)\n",
      "Requirement already satisfied: sniffio>=1.1 in /opt/anaconda3/lib/python3.12/site-packages (from anyio>=3.1.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: argon2-cffi-bindings in /opt/anaconda3/lib/python3.12/site-packages (from argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (21.2.0)\n",
      "Requirement already satisfied: pytz>=2015.7 in /opt/anaconda3/lib/python3.12/site-packages (from babel>=2.10->jupyterlab-server<3,>=2.27.1->notebook) (2024.1)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx>=0.25.0->jupyterlab<4.3,>=4.2.0->notebook) (0.14.0)\n",
      "Requirement already satisfied: appnope in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (0.1.3)\n",
      "Requirement already satisfied: debugpy>=1.6.5 in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (1.6.7)\n",
      "Requirement already satisfied: nest-asyncio in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (1.6.0)\n",
      "Requirement already satisfied: psutil in /opt/anaconda3/lib/python3.12/site-packages (from ipykernel>=6.5.0->jupyterlab<4.3,>=4.2.0->notebook) (5.9.0)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.0 in /opt/anaconda3/lib/python3.12/site-packages (from jedi>=0.16->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jinja2>=3.0.3->jupyter-server<3,>=2.4.0->notebook) (2.1.3)\n",
      "Requirement already satisfied: attrs>=22.2.0 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (23.1.0)\n",
      "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (2023.7.1)\n",
      "Requirement already satisfied: referencing>=0.28.4 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.30.2)\n",
      "Requirement already satisfied: rpds-py>=0.7.1 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema>=4.18.0->jupyterlab-server<3,>=2.27.1->notebook) (0.10.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-client>=7.4.4->jupyter-server<3,>=2.4.0->notebook) (2.9.0.post0)\n",
      "Requirement already satisfied: platformdirs>=2.5 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-core!=5.0.*,>=4.12->jupyter-server<3,>=2.4.0->notebook) (3.10.0)\n",
      "Requirement already satisfied: python-json-logger>=2.0.4 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (2.0.7)\n",
      "Requirement already satisfied: pyyaml>=5.3 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (6.0.1)\n",
      "Requirement already satisfied: rfc3339-validator in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (0.1.4)\n",
      "Requirement already satisfied: rfc3986-validator>=0.1.1 in /opt/anaconda3/lib/python3.12/site-packages (from jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (0.1.1)\n",
      "Requirement already satisfied: beautifulsoup4 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.12.3)\n",
      "Requirement already satisfied: bleach!=5.0.0 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (4.1.0)\n",
      "Requirement already satisfied: defusedxml in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.7.1)\n",
      "Requirement already satisfied: jupyterlab-pygments in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.1.2)\n",
      "Requirement already satisfied: mistune<4,>=2.0.3 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.0.4)\n",
      "Requirement already satisfied: nbclient>=0.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.8.0)\n",
      "Requirement already satisfied: pandocfilters>=1.4.1 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.5.0)\n",
      "Requirement already satisfied: tinycss2 in /opt/anaconda3/lib/python3.12/site-packages (from nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.2.1)\n",
      "Requirement already satisfied: fastjsonschema in /opt/anaconda3/lib/python3.12/site-packages (from nbformat>=5.3.0->jupyter-server<3,>=2.4.0->notebook) (2.16.2)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /opt/anaconda3/lib/python3.12/site-packages (from pexpect>4.3->ipython>=6.1.0->ipywidgets) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /opt/anaconda3/lib/python3.12/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=6.1.0->ipywidgets) (0.2.5)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.0.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/anaconda3/lib/python3.12/site-packages (from requests>=2.31->jupyterlab-server<3,>=2.27.1->notebook) (2.2.2)\n",
      "Requirement already satisfied: executing in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.8.3)\n",
      "Requirement already satisfied: asttokens in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (2.0.5)\n",
      "Requirement already satisfied: pure-eval in /opt/anaconda3/lib/python3.12/site-packages (from stack-data->ipython>=6.1.0->ipywidgets) (0.2.2)\n",
      "Requirement already satisfied: six>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (1.16.0)\n",
      "Requirement already satisfied: webencodings in /opt/anaconda3/lib/python3.12/site-packages (from bleach!=5.0.0->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (0.5.1)\n",
      "Requirement already satisfied: fqdn in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.5.1)\n",
      "Requirement already satisfied: isoduration in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (20.11.0)\n",
      "Requirement already satisfied: jsonpointer>1.13 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (2.1)\n",
      "Requirement already satisfied: uri-template in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.3.0)\n",
      "Requirement already satisfied: webcolors>=1.11 in /opt/anaconda3/lib/python3.12/site-packages (from jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (24.8.0)\n",
      "Requirement already satisfied: cffi>=1.0.1 in /opt/anaconda3/lib/python3.12/site-packages (from argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in /opt/anaconda3/lib/python3.12/site-packages (from beautifulsoup4->nbconvert>=6.4.4->jupyter-server<3,>=2.4.0->notebook) (2.5)\n",
      "Requirement already satisfied: pycparser in /opt/anaconda3/lib/python3.12/site-packages (from cffi>=1.0.1->argon2-cffi-bindings->argon2-cffi>=21.1->jupyter-server<3,>=2.4.0->notebook) (2.21)\n",
      "Requirement already satisfied: arrow>=0.15.0 in /opt/anaconda3/lib/python3.12/site-packages (from isoduration->jsonschema[format-nongpl]>=4.18.0->jupyter-events>=0.9.0->jupyter-server<3,>=2.4.0->notebook) (1.2.3)\n",
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (1.55.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.2.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.27.0)\n",
      "Requirement already satisfied: jiter<1,>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.7.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.5.3)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.4)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2024.8.30)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.14.6 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.14.6)\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers torch\n",
    "!pip install transformers torch scikit-learn\n",
    "!pip install ipywidgets --upgrade\n",
    "!pip install datasets --upgrade\n",
    "!pip install pyarrow --upgrade\n",
    "!pip install huggingface_hub\n",
    "!pip install torch\n",
    "!pip install transformers\n",
    "!pip install scikit-learn\n",
    "!pip install datasets\n",
    "!pip install datasketch\n",
    "!pip install transformers[torch] accelerate\n",
    "!pip install ipywidgets\n",
    "!pip install ipywidgets\n",
    "!jupyter labextension install @jupyter-widgets/jupyterlab-manager\n",
    "!pip install requests\n",
    "!pip install tiktoken\n",
    "!pip install sentencepiece\n",
    "!pip install --upgrade notebook ipywidgets\n",
    "!pip install openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "332e3fe6-4754-4d83-a813-35ca5b318e39",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SentencePiece is installed and ready to use.\n",
      "Using device: cpu\n",
      "Loaded 173 samples.\n",
      "Annotated 100 samples.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at aubmindlab/bert-base-arabertv02 and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "/opt/anaconda3/lib/python3.12/site-packages/transformers/training_args.py:1574: FutureWarning: using `no_cuda` is deprecated and will be removed in version 5.0 of 🤗 Transformers. Use `use_cpu` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting fine-tuning...\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='125' max='125' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [125/125 09:30, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>100</td>\n",
       "      <td>0.094900</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fine-tuning complete.\n",
      "Predicted 73 samples with fine-tuned AraBERT.\n",
      "Validation F1 Score: 1.00\n",
      "Validation Accuracy: 1.00\n",
      "Validation Precision: 1.00\n",
      "Validation Recall: 1.00\n",
      "Confusion Matrix:\n",
      "[[30]]\n",
      "Validation Results of AraBert: {'f1': 1.0, 'accuracy': 1.0, 'precision': 1.0, 'recall': 1.0, 'confusion_matrix': array([[30]])}\n",
      "Filtered dataset contains 0 high-quality samples.\n",
      "Filtered data saved.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/sklearn/metrics/_classification.py:386: UserWarning: A single label was found in 'y_true' and 'y_pred'. For the confusion matrix to have the correct shape, use the 'labels' parameter to pass all known labels.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import ipywidgets as widgets\n",
    "widgets.IntSlider()\n",
    "\n",
    "import json\n",
    "from sklearn.metrics import f1_score\n",
    "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM, Trainer, TrainingArguments, AutoModelForSequenceClassification\n",
    "import torch\n",
    "from transformers import T5Tokenizer, AutoModelForSeq2SeqLM\n",
    "from transformers import AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, confusion_matrix\n",
    "\n",
    "from sentencepiece import SentencePieceProcessor\n",
    "print(\"SentencePiece is installed and ready to use.\")\n",
    "\n",
    "# Device Selection\n",
    "# device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "device = torch.device(\"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "\n",
    "# Configuration\n",
    "config = {\n",
    "    \"model_name\": \"UBC-NLP/araT5-base\",  # Load AraT5 locally\n",
    "    \"fine_tune_model\": \"aubmindlab/bert-base-arabertv02\",  # Model to fine-tune\n",
    "    \"threshold\": 1,  # Minimum score for high-quality content\n",
    "    \n",
    "    \"annotation_samples\": 100,  # Total number of samples annotated with AraT5.\n",
    "    \"validation_samples\": 30,  # Subset of annotation_samples reserved for validation\n",
    "    \"max_samples_to_fine_tune\": 70,  # Maximum annotated samples used for fine-tuning\n",
    "    \n",
    "    \"epochs\": 5,  # Fine-tuning epochs\n",
    "    \"batch_size\": 4,  # Lower the batch size to reduce memory usage\n",
    "}\n",
    "\n",
    "\n",
    "# Arabic Rubric Prompt for Additive Scoring\n",
    "# arabic_prompt = \"\"\"\n",
    "# النص التالي مقتبس من محتوى تعليمي. يرجى تقييم جودة النص بناءً على المعايير التالية:\n",
    "# 1. مدى ملاءمة النص لموضوعات المناهج الدراسية (0-2).\n",
    "# 2. وضوح النص وسهولة فهمه بالنسبة للطلاب (0-2).\n",
    "# 3. عمق المحتوى التعليمي المقدم (0-1).\n",
    "\n",
    "# امنح نقاطًا لكل معيار على حدة، ثم احسب المجموع النهائي (0-5).\n",
    "\n",
    "# النص: \"{text}\"\n",
    "# \"\"\"\n",
    "\n",
    "arabic_prompt = \"\"\"\n",
    "فيما يلي مقتطف من صفحة ويب. قم بتقييم ما إذا كانت الصفحة ذات قيمة تعليمية عالية ويمكن أن تكون مفيدة في بيئة تعليمية لتدريس المستويات من المرحلة الابتدائية إلى المرحلة الإعدادية باستخدام نظام تقييم مكون من 5 نقاط تراكمية وفقًا للمعايير التالية:\n",
    "أضف نقطة واحدة إذا كان المقتطف يقدم بعض المعلومات الأساسية ذات الصلة بالموضوعات التعليمية، حتى لو تضمن محتوى غير ذي صلة أو غير أكاديمي مثل الإعلانات والمواد الترويجية.\n",
    "•\tأضف نقطة أخرى إذا كان المقتطف يتناول بعض العناصر ذات الصلة بالتعليم ولكنه لا يتماشى بشكل وثيق مع المعايير التعليمية. قد يخلط بين المحتوى التعليمي وغير التعليمي، ويقدم نظرة عامة سطحية عن موضوعات قد تكون مفيدة، أو يعرض المعلومات بطريقة غير منظمة وأسلوب كتابة غير واضح.\n",
    "•\tامنح نقطة ثالثة إذا كان المقتطف مناسبًا للاستخدام التعليمي ويقدم مفاهيم رئيسية ذات صلة بالمناهج المدرسية. يكون المحتوى واضحًا ولكنه قد لا يكون شاملاً، أو قد يتضمن بعض المعلومات الزائدة. قد يشبه القسم التمهيدي لكتاب مدرسي أو درس تعليمي بسيط مناسب للتعلم ولكنه يحتوي على بعض القيود مثل معالجة مفاهيم معقدة جدًا لطلاب المرحلة الإعدادية.\n",
    "•\tامنح نقطة رابعة إذا كان المقتطف ذا صلة كبيرة ومفيدًا للأغراض التعليمية لمستوى لا يتجاوز المرحلة الإعدادية، مع أسلوب كتابة واضح ومتسق. يمكن أن يشبه فصلًا من كتاب مدرسي أو درسًا تعليميًا، حيث يقدم محتوى تعليميًا غنيًا، بما في ذلك التمارين والحلول، مع الحد الأدنى من المعلومات غير ذات الصلة، والمفاهيم ليست معقدة للغاية لطلاب هذه المرحلة. يكون المحتوى منظمًا ومركّزًا وقيمًا للتعلم المنهجي.\n",
    "•\tامنح نقطة خامسة إذا كان المقتطف ممتازًا في قيمته التعليمية ومناسبًا تمامًا للتدريس في المرحلة الابتدائية أو الإعدادية. يتبع المقتطف منطقًا تفصيليًا، وأسلوب الكتابة سهل الفهم، ويقدم رؤى عميقة وشاملة حول الموضوع دون أي محتوى غير تعليمي أو معقد.\n",
    "المقتطف: <EXAMPLE>. بعد فحص المقتطف:\n",
    "•\tبرر بإيجاز مجموع النقاط، بحد أقصى 100 كلمة.\n",
    "•\tاختتم بالنقاط الإجمالية بالتنسيق التالي: \"التقييم التعليمي: <مجموع النقاط>\".\n",
    "\"\"\"\n",
    "\n",
    "# Modify the dataset to use a Dataset class that provides a dictionary-like format. \n",
    "# Replace the torch.utils.data.TensorDataset with a Dataset object.\n",
    "from torch.utils.data import Dataset\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, texts, labels, tokenizer, max_length=256):\n",
    "        self.texts = texts\n",
    "        self.labels = labels\n",
    "        self.tokenizer = tokenizer\n",
    "        self.max_length = max_length\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.texts)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        # Tokenize the text\n",
    "        text = self.texts[idx]\n",
    "        label = self.labels[idx]\n",
    "        encoding = self.tokenizer(\n",
    "            text,\n",
    "            max_length=self.max_length,\n",
    "            padding=\"max_length\",\n",
    "            truncation=True,\n",
    "            return_tensors=\"pt\",\n",
    "        )\n",
    "        # Flatten tensors and include labels\n",
    "        item = {key: val.squeeze(0) for key, val in encoding.items()}\n",
    "        item[\"labels\"] = torch.tensor(label, dtype=torch.long)\n",
    "        return item\n",
    "\n",
    "\n",
    "# Step 1: Load Dataset\n",
    "def load_dataset(file_path):\n",
    "    with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "        data = json.load(file)\n",
    "    # Adjust to handle `text` and `metadata`\n",
    "    return [{\"text\": item[\"text\"], \"metadata\": item[\"metadata\"]} for item in data]\n",
    "\n",
    "\n",
    "# Step 2: Annotate Data Locally with AraT5\n",
    "def annotate_samples(samples, model_name):\n",
    "    tokenizer = T5Tokenizer.from_pretrained(model_name, legacy=False)\n",
    "    model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n",
    "    annotated_data = []\n",
    "\n",
    "    for sample in samples:\n",
    "        text = sample[\"text\"]\n",
    "        prompt = arabic_prompt.format(text=text)\n",
    "\n",
    "        # Encode input text and generate output\n",
    "        inputs = tokenizer(prompt, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        outputs = model.generate(**inputs, max_length=100)\n",
    "        result = tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
    "\n",
    "        scores = extract_scores(result)\n",
    "        annotated_data.append({\"text\": text, \"scores\": scores, \"metadata\": sample[\"metadata\"]})\n",
    "\n",
    "    return annotated_data\n",
    "\n",
    "# Helper Function to Extract Scores\n",
    "def extract_scores(output):\n",
    "    lines = output.split(\"\\n\")\n",
    "    scores = {}\n",
    "    for line in lines:\n",
    "        if \"ملاءمة\" in line:\n",
    "            scores[\"relevance\"] = int(line.split(\":\")[-1].strip())\n",
    "        elif \"وضوح\" in line:\n",
    "            scores[\"clarity\"] = int(line.split(\":\")[-1].strip())\n",
    "        elif \"عمق\" in line:\n",
    "            scores[\"depth\"] = int(line.split(\":\")[-1].strip())\n",
    "    total = sum(scores.values())\n",
    "    scores[\"total\"] = total\n",
    "    return scores\n",
    "\n",
    "# Step 3: Fine-Tune AraBERT\n",
    "def fine_tune_arabert(train_data, tokenizer, model):\n",
    "    texts = [item[\"text\"] for item in train_data]\n",
    "    labels = [item[\"scores\"][\"total\"] for item in train_data]\n",
    "\n",
    "    # Use CustomDataset\n",
    "    dataset = CustomDataset(texts, labels, tokenizer)\n",
    "\n",
    "    # Move model to device\n",
    "    model.to(device)\n",
    "\n",
    "    training_args = TrainingArguments(\n",
    "    output_dir=\"./results\",\n",
    "    num_train_epochs=config[\"epochs\"],\n",
    "    per_device_train_batch_size=config[\"batch_size\"],\n",
    "    save_steps=10_000,\n",
    "    save_total_limit=2,\n",
    "    no_cuda=True,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps=100  # Log progress every 100 steps\n",
    "    )\n",
    "\n",
    "\n",
    "    trainer = Trainer(\n",
    "        model=model,\n",
    "        args=training_args,\n",
    "        train_dataset=dataset,\n",
    "    )\n",
    "\n",
    "    print(\"Starting fine-tuning...\")\n",
    "    trainer.train()\n",
    "    print(\"Fine-tuning complete.\")\n",
    "\n",
    "\n",
    "# Step 4: Validate and Benchmark\n",
    "def validate_model(validation_data, model, tokenizer):\n",
    "    # Ensure the model is on the correct device\n",
    "    model.to(device)\n",
    "\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for item in validation_data:\n",
    "        text = item[\"text\"]\n",
    "        true_labels.append(item[\"scores\"][\"total\"])  # Actual labels\n",
    "\n",
    "        # Prepare inputs and move to the same device as the model\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = logits.argmax(dim=-1).item()  # Predicted label\n",
    "        predicted_labels.append(predicted_label)\n",
    "\n",
    "    # Compute metrics\n",
    "    f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    precision = precision_score(true_labels, predicted_labels, average=\"macro\", zero_division=0)\n",
    "    recall = recall_score(true_labels, predicted_labels, average=\"macro\", zero_division=0)\n",
    "    conf_matrix = confusion_matrix(true_labels, predicted_labels)\n",
    "\n",
    "    # Print metrics directly here\n",
    "    print(f\"Validation F1 Score: {f1:.2f}\")\n",
    "    print(f\"Validation Accuracy: {accuracy:.2f}\")\n",
    "    print(f\"Validation Precision: {precision:.2f}\")\n",
    "    print(f\"Validation Recall: {recall:.2f}\")\n",
    "    print(\"Confusion Matrix:\")\n",
    "    print(conf_matrix)\n",
    "\n",
    "    # Return all metrics as a dictionary for further use if needed\n",
    "    return {\n",
    "        \"f1\": f1,\n",
    "        \"accuracy\": accuracy,\n",
    "        \"precision\": precision,\n",
    "        \"recall\": recall,\n",
    "        \"confusion_matrix\": conf_matrix,\n",
    "    }\n",
    "\n",
    "def predict_with_arabert(unlabeled_data, model, tokenizer):\n",
    "    \"\"\"\n",
    "    Use the fine-tuned AraBERT model to predict labels for the rest of the dataset.\n",
    "    Args:\n",
    "        unlabeled_data: List of unannotated samples.\n",
    "        model: Fine-tuned AraBERT model.\n",
    "        tokenizer: Tokenizer for AraBERT.\n",
    "    Returns:\n",
    "        List of samples with predicted scores.\n",
    "    \"\"\"\n",
    "    model.eval()  # Set the model to evaluation mode\n",
    "    model.to(device)\n",
    "\n",
    "    predictions = []\n",
    "\n",
    "    for sample in unlabeled_data:\n",
    "        text = sample[\"text\"]\n",
    "        inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "        inputs = {key: val.to(device) for key, val in inputs.items()}\n",
    "\n",
    "        with torch.no_grad():\n",
    "            outputs = model(**inputs)\n",
    "        logits = outputs.logits\n",
    "        predicted_label = logits.argmax(dim=-1).item()\n",
    "\n",
    "        predictions.append({\"text\": text, \"predicted_score\": predicted_label, \"metadata\": sample[\"metadata\"]})\n",
    "\n",
    "    return predictions\n",
    "\n",
    "\n",
    "def filter_dataset(annotated_data, threshold):\n",
    "    \"\"\"\n",
    "    Filters the dataset to retain high-quality samples.\n",
    "    Args:\n",
    "        annotated_data: List of annotated samples or predictions.\n",
    "        threshold: Minimum score for filtering.\n",
    "    Returns:\n",
    "        Filtered dataset.\n",
    "    \"\"\"\n",
    "    return [\n",
    "        doc for doc in annotated_data\n",
    "        if doc[\"predicted_score\"] >= threshold  # Removed 'language_score' filter\n",
    "    ]\n",
    "\n",
    "# Main Pipeline\n",
    "def main_pipeline():\n",
    "    # Step 1: Load the dataset\n",
    "    dataset = load_dataset(\"/Users/ameeraattiah/Desktop/arabicweb24/jeje.json\")\n",
    "    print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "    # Step 2: Annotate a sample of the dataset with AraT5\n",
    "    sample_data = dataset[:config[\"annotation_samples\"]]\n",
    "    annotated_data = annotate_samples(sample_data, config[\"model_name\"])\n",
    "    print(f\"Annotated {len(annotated_data)} samples.\")\n",
    "\n",
    "    # Step 3: Fine-tune AraBERT on the synthetic annotations\n",
    "    tokenizer = AutoTokenizer.from_pretrained(config[\"fine_tune_model\"])\n",
    "    model = AutoModelForSequenceClassification.from_pretrained(config[\"fine_tune_model\"], num_labels=6)\n",
    "    fine_tune_arabert(annotated_data, tokenizer, model)\n",
    "\n",
    "    # Step 4: Predict the rest of the dataset with fine-tuned AraBERT\n",
    "    remaining_data = dataset[config[\"annotation_samples\"]:]\n",
    "    predictions = predict_with_arabert(remaining_data, model, tokenizer)\n",
    "    print(f\"Predicted {len(predictions)} samples with fine-tuned AraBERT.\")\n",
    "\n",
    "    # Step 5: Validate fine-tuned AraBERT\n",
    "    validation_data = annotated_data[:config[\"validation_samples\"]]\n",
    "    validation_results = validate_model(validation_data, model, tokenizer)\n",
    "    \n",
    "    # Print evaluation metrics\n",
    "    print(f\"Validation Results of AraBert: {validation_results}\")\n",
    "\n",
    "\n",
    "    # Step 6: Filter high-quality samples\n",
    "    filtered_data = filter_dataset(predictions, config[\"threshold\"])\n",
    "    print(f\"Filtered dataset contains {len(filtered_data)} high-quality samples.\")\n",
    "\n",
    "    # Step7: Save annotated and filtered dataset\n",
    "    with open(\"/Users/ameeraattiah/Desktop/arabicweb24/jeje-edu.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "        json.dump(filtered_data, file, ensure_ascii=False, indent=4)\n",
    "    print(\"Filtered data saved.\")\n",
    "\n",
    "# Run the pipeline\n",
    "main_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7aba9b47-434d-4aaa-8ace-9505a0002e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import ipywidgets as widgets\n",
    "# widgets.IntSlider()\n",
    "\n",
    "# import json\n",
    "# import requests\n",
    "# from sklearn.metrics import f1_score\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, Trainer, TrainingArguments\n",
    "# import torch\n",
    "\n",
    "# # Configuration\n",
    "# config = {\n",
    "#     \"api_url\": \"https://api.jais.ai/v1/generate\",  # Replace with the actual Jais API endpoint\n",
    "#     \"api_key\": \"your_api_key_here\",  # Replace with your Jais API key\n",
    "#     \"fine_tune_model\": \"aubmindlab/bert-base-arabertv02\",  # Model to fine-tune\n",
    "#     \"threshold\": 3,  # Minimum score for high-quality content\n",
    "#     \"annotation_samples\": 500,  # Number of samples to annotate for testing\n",
    "#     \"validation_samples\": 100,  # Number of samples for validation\n",
    "#     \"max_samples_to_fine_tune\": 1000,  # Max samples for fine-tuning\n",
    "#     \"epochs\": 20,  # Fine-tuning epochs\n",
    "#     \"batch_size\": 8,  # Batch size for fine-tuning\n",
    "# }\n",
    "\n",
    "# # Arabic Rubric Prompt for Additive Scoring\n",
    "# arabic_prompt = \"\"\"\n",
    "# النص التالي مقتبس من محتوى تعليمي. يرجى تقييم جودة النص بناءً على المعايير التالية:\n",
    "# 1. مدى ملاءمة النص لموضوعات المناهج الدراسية (0-2).\n",
    "# 2. وضوح النص وسهولة فهمه بالنسبة للطلاب (0-2).\n",
    "# 3. عمق المحتوى التعليمي المقدم (0-1).\n",
    "\n",
    "# امنح نقاطًا لكل معيار على حدة، ثم احسب المجموع النهائي (0-5).\n",
    "\n",
    "# النص: \"{text}\"\n",
    "# \"\"\"\n",
    "\n",
    "# # Step 1: Load Dataset\n",
    "# def load_dataset(file_path):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#         data = json.load(file)\n",
    "#     return [{\"text\": item[\"text\"], \"metadata\": item[\"metadata\"]} for item in data]\n",
    "\n",
    "# # Step 2: Annotate Data with API\n",
    "# def annotate_samples(samples, api_url, api_key):\n",
    "#     headers = {\"Authorization\": f\"Bearer {api_key}\", \"Content-Type\": \"application/json\"}\n",
    "#     annotated_data = []\n",
    "\n",
    "#     for sample in samples:\n",
    "#         text = sample[\"text\"]\n",
    "#         prompt = arabic_prompt.format(text=text)\n",
    "#         payload = {\n",
    "#             \"prompt\": prompt,\n",
    "#             \"max_tokens\": 100,  # Adjust as needed\n",
    "#             \"temperature\": 0.7,  # Optional: Controls randomness\n",
    "#         }\n",
    "#         response = requests.post(api_url, headers=headers, json=payload)\n",
    "\n",
    "#         if response.status_code == 200:\n",
    "#             result = response.json()[\"generated_text\"]\n",
    "#             scores = extract_scores(result)\n",
    "#             annotated_data.append({\"text\": text, \"scores\": scores, \"metadata\": sample[\"metadata\"]})\n",
    "#         else:\n",
    "#             print(f\"Error: {response.status_code}, {response.text}\")\n",
    "\n",
    "#     return annotated_data\n",
    "\n",
    "# # Helper Function to Extract Scores\n",
    "# def extract_scores(output):\n",
    "#     lines = output.split(\"\\n\")\n",
    "#     scores = {}\n",
    "#     for line in lines:\n",
    "#         if \"ملاءمة\" in line:\n",
    "#             scores[\"relevance\"] = int(line.split(\":\")[-1].strip())\n",
    "#         elif \"وضوح\" in line:\n",
    "#             scores[\"clarity\"] = int(line.split(\":\")[-1].strip())\n",
    "#         elif \"عمق\" in line:\n",
    "#             scores[\"depth\"] = int(line.split(\":\")[-1].strip())\n",
    "#     total = sum(scores.values())\n",
    "#     scores[\"total\"] = total\n",
    "#     return scores\n",
    "\n",
    "# # Step 3: Fine-Tune AraBERT\n",
    "# def fine_tune_arabert(train_data, tokenizer, model):\n",
    "#     texts = [item[\"text\"] for item in train_data]\n",
    "#     labels = [item[\"scores\"][\"total\"] for item in train_data]\n",
    "\n",
    "#     encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "#     dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], torch.tensor(labels))\n",
    "\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"./results\",\n",
    "#         num_train_epochs=config[\"epochs\"],\n",
    "#         per_device_train_batch_size=config[\"batch_size\"],\n",
    "#         save_steps=10_000,\n",
    "#         save_total_limit=2,\n",
    "#         logging_dir=\"./logs\",\n",
    "#     )\n",
    "\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset,\n",
    "#     )\n",
    "\n",
    "#     print(\"Starting fine-tuning...\")\n",
    "#     trainer.train()\n",
    "#     print(\"Fine-tuning complete.\")\n",
    "\n",
    "# # Step 4: Validate and Benchmark\n",
    "# def validate_model(validation_data, model, tokenizer):\n",
    "#     true_labels = []\n",
    "#     predicted_labels = []\n",
    "#     for item in validation_data:\n",
    "#         text = item[\"text\"]\n",
    "#         true_labels.append(item[\"scores\"][\"total\"])\n",
    "#         inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         predicted_label = logits.argmax(dim=-1).item()\n",
    "#         predicted_labels.append(predicted_label)\n",
    "\n",
    "#     f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
    "#     print(f\"Validation F1 Score: {f1:.2f}\")\n",
    "#     return f1\n",
    "\n",
    "# # Main Pipeline\n",
    "# def main_pipeline():\n",
    "#     # Load dataset\n",
    "#     dataset = load_dataset(\"/Users/ameeraattiah/Desktop/warc/meero_cleaned.json\")\n",
    "#     print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "#     # Annotate samples using the Jais API\n",
    "#     annotation_samples = dataset[:config[\"annotation_samples\"]]\n",
    "#     annotated_data = annotate_samples(annotation_samples, config[\"api_url\"], config[\"api_key\"])\n",
    "#     print(f\"Annotated {len(annotated_data)} samples.\")\n",
    "\n",
    "#     # Fine-tune AraBERT\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(config[\"fine_tune_model\"])\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(config[\"fine_tune_model\"], num_labels=6)\n",
    "#     fine_tune_arabert(annotated_data[:config[\"max_samples_to_fine_tune\"]], tokenizer, model)\n",
    "\n",
    "#     # Validate the model\n",
    "#     validation_data = annotated_data[:config[\"validation_samples\"]]\n",
    "#     f1 = validate_model(validation_data, model, tokenizer)\n",
    "\n",
    "#     # Save annotated data\n",
    "#     with open(\"/Users/ameeraattiah/Desktop/warc/annotated_meero.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         json.dump(annotated_data, file, ensure_ascii=False, indent=4)\n",
    "#     print(\"Saved annotated dataset.\")\n",
    "\n",
    "# # Run the pipeline\n",
    "# main_pipeline()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "ec980c2f-4e0a-4dc6-8225-dfa5eeabbd8a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The history saving thread hit an unexpected error (OperationalError('unable to open database file')).History will not be written to the database.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fb498c15dd134741b662427ac6cedfcd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading shards:   0%|          | 0/6 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py:653: UserWarning: Not enough free disk space to download the file. The expected file size is: 9963.63 MB. The target location /Users/ameeraattiah/.cache/huggingface/hub/models--inceptionai--jais-13b/blobs only has 31.99 MB free disk space.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "844bbe6c845b45c8b414be42e7e42f35",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "pytorch_model-00003-of-00006.bin:  96%|#########5| 9.53G/9.96G [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "ValueError",
     "evalue": "Could not load model inceptionai/jais-13b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3974, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n                      ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1011, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1545, in _download_to_tmp_and_move\n    http_get(\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 457, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 25\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;66;03m# Configuration\u001b[39;00m\n\u001b[1;32m     14\u001b[0m config \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m     15\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mannotator_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minceptionai/jais-13b\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Correct Jais model name\u001b[39;00m\n\u001b[1;32m     16\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mfine_tune_model\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124maubmindlab/bert-base-arabertv02\u001b[39m\u001b[38;5;124m\"\u001b[39m,  \u001b[38;5;66;03m# Model to fine-tune\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     23\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbenchmark_tasks\u001b[39m\u001b[38;5;124m\"\u001b[39m: [\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mQA\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCurriculum\u001b[39m\u001b[38;5;124m\"\u001b[39m],  \u001b[38;5;66;03m# Placeholder for benchmarking\u001b[39;00m\n\u001b[1;32m     24\u001b[0m }\n\u001b[0;32m---> 25\u001b[0m model_pipeline \u001b[38;5;241m=\u001b[39m pipeline(\n\u001b[1;32m     26\u001b[0m     \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext-generation\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     27\u001b[0m     model\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124minceptionai/jais-13b\u001b[39m\u001b[38;5;124m\"\u001b[39m, \n\u001b[1;32m     28\u001b[0m     trust_remote_code\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m     29\u001b[0m )\n\u001b[1;32m     32\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtransformers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m pipeline\n\u001b[1;32m     35\u001b[0m \u001b[38;5;66;03m# Arabic Rubric Prompt for Additive Scoring\u001b[39;00m\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/__init__.py:926\u001b[0m, in \u001b[0;36mpipeline\u001b[0;34m(task, model, config, tokenizer, feature_extractor, image_processor, processor, framework, revision, use_fast, token, device, device_map, torch_dtype, trust_remote_code, model_kwargs, pipeline_class, **kwargs)\u001b[0m\n\u001b[1;32m    924\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(model, \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    925\u001b[0m     model_classes \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtf\u001b[39m\u001b[38;5;124m\"\u001b[39m], \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m: targeted_task[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mpt\u001b[39m\u001b[38;5;124m\"\u001b[39m]}\n\u001b[0;32m--> 926\u001b[0m     framework, model \u001b[38;5;241m=\u001b[39m infer_framework_load_model(\n\u001b[1;32m    927\u001b[0m         model,\n\u001b[1;32m    928\u001b[0m         model_classes\u001b[38;5;241m=\u001b[39mmodel_classes,\n\u001b[1;32m    929\u001b[0m         config\u001b[38;5;241m=\u001b[39mconfig,\n\u001b[1;32m    930\u001b[0m         framework\u001b[38;5;241m=\u001b[39mframework,\n\u001b[1;32m    931\u001b[0m         task\u001b[38;5;241m=\u001b[39mtask,\n\u001b[1;32m    932\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mhub_kwargs,\n\u001b[1;32m    933\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mmodel_kwargs,\n\u001b[1;32m    934\u001b[0m     )\n\u001b[1;32m    936\u001b[0m model_config \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\n\u001b[1;32m    937\u001b[0m hub_kwargs[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m_commit_hash\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mconfig\u001b[38;5;241m.\u001b[39m_commit_hash\n",
      "File \u001b[0;32m/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py:302\u001b[0m, in \u001b[0;36minfer_framework_load_model\u001b[0;34m(model, config, model_classes, task, framework, **model_kwargs)\u001b[0m\n\u001b[1;32m    300\u001b[0m         \u001b[38;5;28;01mfor\u001b[39;00m class_name, trace \u001b[38;5;129;01min\u001b[39;00m all_traceback\u001b[38;5;241m.\u001b[39mitems():\n\u001b[1;32m    301\u001b[0m             error \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mwhile loading with \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m, an error is thrown:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mtrace\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 302\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    303\u001b[0m             \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCould not load model \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m with any of the following classes: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mclass_tuple\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m. See the original errors:\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00merror\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    304\u001b[0m         )\n\u001b[1;32m    306\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m framework \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    307\u001b[0m     framework \u001b[38;5;241m=\u001b[39m infer_framework(model\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m)\n",
      "\u001b[0;31mValueError\u001b[0m: Could not load model inceptionai/jais-13b with any of the following classes: (<class 'transformers.models.auto.modeling_auto.AutoModelForCausalLM'>,). See the original errors:\n\nwhile loading with AutoModelForCausalLM, an error is thrown:\nTraceback (most recent call last):\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/pipelines/base.py\", line 289, in infer_framework_load_model\n    model = model_class.from_pretrained(model, **kwargs)\n            ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/models/auto/auto_factory.py\", line 559, in from_pretrained\n    return model_class.from_pretrained(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/modeling_utils.py\", line 3974, in from_pretrained\n    resolved_archive_file, sharded_metadata = get_checkpoint_shard_files(\n                                              ^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py\", line 1098, in get_checkpoint_shard_files\n    cached_filename = cached_file(\n                      ^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/transformers/utils/hub.py\", line 403, in cached_file\n    resolved_file = hf_hub_download(\n                    ^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/utils/_validators.py\", line 114, in _inner_fn\n    return fn(*args, **kwargs)\n           ^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 862, in hf_hub_download\n    return _hf_hub_download_to_cache_dir(\n           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1011, in _hf_hub_download_to_cache_dir\n    _download_to_tmp_and_move(\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 1545, in _download_to_tmp_and_move\n    http_get(\n  File \"/opt/anaconda3/lib/python3.12/site-packages/huggingface_hub/file_download.py\", line 457, in http_get\n    temp_file.write(chunk)\nOSError: [Errno 28] No space left on device\n\n\n"
     ]
    }
   ],
   "source": [
    "# import json\n",
    "# import numpy as np\n",
    "# from sklearn.linear_model import LinearRegression\n",
    "# from sklearn.metrics import f1_score\n",
    "# from transformers import AutoTokenizer, AutoModelForSequenceClassification, pipeline, Trainer, TrainingArguments\n",
    "# import torch\n",
    "# import sys\n",
    "# import os\n",
    "# from datatrove.data import Document  # Import the Document class\n",
    "\n",
    "\n",
    "\n",
    "# # Configuration\n",
    "# config = {\n",
    "#     \"annotator_model\": \"inceptionai/jais-13b\",  # Correct Jais model name\n",
    "#     \"fine_tune_model\": \"aubmindlab/bert-base-arabertv02\",  # Model to fine-tune\n",
    "#     \"threshold\": 3,  # Minimum score for high-quality content\n",
    "#     \"annotation_samples\": 500,  # Number of samples to annotate for testing\n",
    "#     \"validation_samples\": 100,  # Number of samples for validation\n",
    "#     \"max_samples_to_fine_tune\": 1000,  # Max samples for fine-tuning\n",
    "#     \"epochs\": 20,  # Fine-tuning epochs\n",
    "#     \"batch_size\": 8,  # Batch size for fine-tuning\n",
    "#     \"benchmark_tasks\": [\"QA\", \"Curriculum\"],  # Placeholder for benchmarking\n",
    "# }\n",
    "# model_pipeline = pipeline(\n",
    "#     \"text-generation\", \n",
    "#     model=\"inceptionai/jais-13b\", \n",
    "#     trust_remote_code=True\n",
    "# )\n",
    "\n",
    "\n",
    "# from transformers import pipeline\n",
    "\n",
    "\n",
    "# # Arabic Rubric Prompt for Additive Scoring\n",
    "# arabic_prompt = \"\"\"\n",
    "# النص التالي مقتبس من محتوى تعليمي. يرجى تقييم جودة النص بناءً على المعايير التالية:\n",
    "# 1. مدى ملاءمة النص لموضوعات المناهج الدراسية (0-2).\n",
    "# 2. وضوح النص وسهولة فهمه بالنسبة للطلاب (0-2).\n",
    "# 3. عمق المحتوى التعليمي المقدم (0-1).\n",
    "\n",
    "# امنح نقاطًا لكل معيار على حدة، ثم احسب المجموع النهائي (0-5).\n",
    "\n",
    "# النص: \"{text}\"\n",
    "# \"\"\"\n",
    "\n",
    "# # Step 1: Load Dataset\n",
    "# def load_dataset(file_path):\n",
    "#     with open(file_path, \"r\", encoding=\"utf-8\") as file:\n",
    "#         data = json.load(file)\n",
    "#     return [{\"text\": item[\"text\"], \"metadata\": item[\"metadata\"]} for item in data]\n",
    "\n",
    "# # Step 2: Annotate Data with Additive Scoring\n",
    "# def annotate_samples(samples, model_pipeline):\n",
    "#     annotated_data = []\n",
    "#     for sample in samples:\n",
    "#         text = sample[\"text\"]\n",
    "#         prompt = arabic_prompt.format(text=text)\n",
    "#         result = model_pipeline(prompt)[0]\n",
    "#         scores = result[\"generated_text\"].split()  # Assume the model generates scores per criterion\n",
    "#         relevance, clarity, depth = map(int, scores[:3])  # Extract individual scores\n",
    "#         total_score = relevance + clarity + depth\n",
    "#         annotated_data.append({\"text\": text, \"score\": total_score, \"metadata\": sample[\"metadata\"]})\n",
    "#     return annotated_data\n",
    "\n",
    "# # Step 3: Fine-Tune AraBERT\n",
    "# def fine_tune_arabert(train_data, tokenizer, model):\n",
    "#     # Prepare data\n",
    "#     texts = [item[\"text\"] for item in train_data]\n",
    "#     labels = [item[\"score\"] for item in train_data]\n",
    "\n",
    "#     # Tokenize data\n",
    "#     encodings = tokenizer(texts, truncation=True, padding=True, max_length=512, return_tensors=\"pt\")\n",
    "#     dataset = torch.utils.data.TensorDataset(encodings[\"input_ids\"], encodings[\"attention_mask\"], torch.tensor(labels))\n",
    "\n",
    "#     # Define training arguments\n",
    "#     training_args = TrainingArguments(\n",
    "#         output_dir=\"./results\",\n",
    "#         num_train_epochs=config[\"epochs\"],\n",
    "#         per_device_train_batch_size=config[\"batch_size\"],\n",
    "#         save_steps=10_000,\n",
    "#         save_total_limit=2,\n",
    "#         logging_dir=\"./logs\",\n",
    "#     )\n",
    "\n",
    "#     # Trainer setup\n",
    "#     trainer = Trainer(\n",
    "#         model=model,\n",
    "#         args=training_args,\n",
    "#         train_dataset=dataset,\n",
    "#     )\n",
    "\n",
    "#     print(\"Starting fine-tuning...\")\n",
    "#     trainer.train()\n",
    "#     print(\"Fine-tuning complete.\")\n",
    "\n",
    "# # Step 4: Validate and Benchmark\n",
    "# def validate_model(validation_data, model, tokenizer):\n",
    "#     true_labels = []\n",
    "#     predicted_labels = []\n",
    "#     for item in validation_data:\n",
    "#         text = item[\"text\"]\n",
    "#         true_labels.append(item[\"score\"])\n",
    "#         inputs = tokenizer(text, return_tensors=\"pt\", truncation=True, max_length=512)\n",
    "#         with torch.no_grad():\n",
    "#             outputs = model(**inputs)\n",
    "#         logits = outputs.logits\n",
    "#         predicted_label = logits.argmax(dim=-1).item()\n",
    "#         predicted_labels.append(predicted_label)\n",
    "\n",
    "#     # Calculate F1 Score\n",
    "#     f1 = f1_score(true_labels, predicted_labels, average=\"macro\")\n",
    "#     print(f\"Validation F1 Score: {f1:.2f}\")\n",
    "#     return f1\n",
    "\n",
    "# # Step 5: Ablation Studies and Threshold Optimization\n",
    "# def ablation_studies(filtered_data):\n",
    "#     thresholds = range(1, 6)\n",
    "#     best_threshold = None\n",
    "#     best_f1 = 0\n",
    "\n",
    "#     for threshold in thresholds:\n",
    "#         current_data = [item for item in filtered_data if item[\"score\"] >= threshold]\n",
    "#         print(f\"Threshold: {threshold}, Data Size: {len(current_data)}\")\n",
    "#         f1 = validate_model(current_data, model, tokenizer)\n",
    "#         if f1 > best_f1:\n",
    "#             best_f1 = f1\n",
    "#             best_threshold = threshold\n",
    "\n",
    "#     print(f\"Optimal Threshold: {best_threshold}, Best F1: {best_f1:.2f}\")\n",
    "# # Main Pipeline\n",
    "# def main_pipeline():\n",
    "#     # Load dataset\n",
    "#     dataset = load_dataset(\"/Users/ameeraattiah/Desktop/warc/meero_cleaned.json\")\n",
    "#     print(f\"Loaded {len(dataset)} samples.\")\n",
    "\n",
    "#     # Annotate samples using the correct Jais model\n",
    "#     annotation_samples = dataset[:config[\"annotation_samples\"]]\n",
    "#     model_pipeline = pipeline(\n",
    "#         \"text-generation\", \n",
    "#         model=config[\"annotator_model\"], \n",
    "#         trust_remote_code=True\n",
    "#     )\n",
    "#     annotated_data = annotate_samples(annotation_samples, model_pipeline)\n",
    "#     print(f\"Annotated {len(annotated_data)} samples.\")\n",
    "\n",
    "#     # Fine-tune AraBERT\n",
    "#     tokenizer = AutoTokenizer.from_pretrained(config[\"fine_tune_model\"])\n",
    "#     model = AutoModelForSequenceClassification.from_pretrained(config[\"fine_tune_model\"], num_labels=6)\n",
    "#     fine_tune_arabert(annotated_data[:config[\"max_samples_to_fine_tune\"]], tokenizer, model)\n",
    "\n",
    "#     # Validate the model\n",
    "#     validation_data = annotated_data[:config[\"validation_samples\"]]\n",
    "#     f1 = validate_model(validation_data, model, tokenizer)\n",
    "\n",
    "#     # Apply ablation studies\n",
    "#     ablation_studies(annotated_data)\n",
    "\n",
    "#     # Save annotated data\n",
    "#     with open(\"/Users/ameeraattiah/Desktop/warc/annotated_meero.json\", \"w\", encoding=\"utf-8\") as file:\n",
    "#         json.dump(annotated_data, file, ensure_ascii=False, indent=4)\n",
    "#     print(\"Saved annotated dataset.\")\n",
    "\n",
    "# # Run the pipeline\n",
    "# main_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
